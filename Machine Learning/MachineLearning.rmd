---
title: "SoloLearn Machine Learning"
author: "Ken Harmon"
date: "`r format(Sys.time(), '%Y %B %d')`"
output:
  html_document:
    keep_md: yes
    code_folding: hide
    fig_height: 6
    fig_width: 12
    fig_align: center
  pdf_document: default
editor_options:
  chunk_output_type: console
---
 
https://www.sololearn.com/Play/machine-learning

# {.tabset .tabset-fade}

```{r, echo=FALSE}
library(reticulate)
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE)
knitr::knit_engines$set(python = reticulate::eng_python)
reticulate::repl_python()

```

```{r}
#py_install("scikit-learn")
#py_install("sqlalchemy")
```

Statistics with Python

We can calculate all of these operations with Python. We will use the Python package numpy. We will use numpy more later for manipulating arrays, but for now we will just use a few functions for statistical calculations: mean, median, percentile, std, var.

First we import the package. It is standard practice to nickname numpy as np.
import numpy as np

Let’s initialize the variable data to have the list of ages.
data = [15, 16, 18, 19, 22, 24, 29, 30, 34]

Now we can use the numpy functions. For the mean, median, standard deviation and variance functions, we just pass in the data list. For the percentile function we pass the data list and the percentile (as a number between 0 and 100).

Try it now:
import numpy as np

data = [15, 16, 18, 19, 22, 24, 29, 30, 34]

print("mean:", np.mean(data))
print("median:", np.median(data))
print("50th percentile (median):", np.percentile(data, 50))
print("25th percentile:", np.percentile(data, 25))
print("75th percentile:", np.percentile(data, 75))
print("standard deviation:", np.std(data))
print("variance:", np.var(data))

```{python}
import numpy as np

data = [15, 16, 18, 19, 22, 24, 29, 30, 34]

print("mean:", np.mean(data))
print("median:", np.median(data))
print("50th percentile (median):", np.percentile(data, 50))
print("25th percentile:", np.percentile(data, 25))
print("75th percentile:", np.percentile(data, 75))
print("standard deviation:", np.std(data))
print("variance:", np.var(data))
```

Read in Your Data

We need to start by importing Pandas. It's standard practice to nickname it pd so that it's faster to type later on.
import pandas as pd 

We will be working with a dataset of Titanic passengers. For each passenger, we’ll have some data on them as well as whether or not they survived the crash.

Our data is stored as CSV (comma-separated values) file. The titanic.csv file is below. The first line is the header and then each subsequent line is the data for a single passenger.
Survived, Pclass, Sex, Age, Siblings/Spouses, Parents/Children, Fare
0, 3, male, 22.0, 1, 0, 7.25
1, 1, female, 38.0, 1, 0, 71.2833
1, 3, female, 26.0, 0, 0, 7.925
1, 1, female, 35.0, 1, 0, 53.1

We're going to pull the data into pandas so we can view it as a DataFrame.

The read_csv function takes a file in csv format and converts it to a Pandas DataFrame.
df = pd.read_csv('titanic.csv')

The object df is now our pandas dataframe with the Titanic dataset. Now we can use the head method to look at the data.
The head method returns the first 5 rows of the DataFrame.
print(df.head())

Run this code to see the results:
import pandas as pd
df = pd.read_csv('https://sololearn.com/uploads/files/titanic.csv')
print(df.head())

```{python}
import pandas as pd 

df = pd.read_csv('titanic.csv')

print(df.head())

```

Summarize the Data

Usually our data is much too big for us to be able to display it all.
Looking at the first few rows is the first step to understanding our data, but then we want to look at some summary statistics.
In pandas, we can use the describe method. It returns a table of statistics about the columns.
print(df.describe())
We add a line in the code below to force python to display all 6 columns. Without the line, it will abbreviate the results.

Run this code to see the results:
import pandas as pd
pd.options.display.max_columns = 6
df = pd.read_csv('https://sololearn.com/uploads/files/titanic.csv')
print(df.describe())

```{python}
import pandas as pd
pd.options.display.max_columns = 6
df = pd.read_csv('titanic.csv')
print(df.describe())
```

Selecting a Single Column

We often will only want to deal with some of the columns that we have in our dataset. To select a single column, we use the square brackets and the column name.

In this example, we're selecting just the column with the passenger fares.
col = df['Fare']
print(col)

The result is what we call a Pandas Series.
A series is like a DataFrame, but it's just a single column.

Run this code to see the results:
import pandas as pd
df = pd.read_csv('https://sololearn.com/uploads/files/titanic.csv')
col = df['Fare']
print(col) 

```{python}
import pandas as pd
df = pd.read_csv('titanic.csv')
col = df['Fare']
print(col) 
```

Selecting Multiple Columns

We can also select multiple columns from our original DataFrame, creating a smaller DataFrame.
We're going to select just the Age, Sex, and Survived columns from our original DataFrame.

We put these values in a list as follows:
['Age', 'Sex', 'Survived']

Now we use that list inside of the bracket notation df[...] When printing a large DataFrame that’s too big to display, you can use the head method to print just the first 5 rows.
small_df = df[['Age', 'Sex', 'Survived']]
print(small_df.head()) 

Run this code to see the results:
import pandas as pd
df = pd.read_csv('https://sololearn.com/uploads/files/titanic.csv')
small_df = df[['Age', 'Sex', 'Survived']]
print(small_df.head())

```{python}
import pandas as pd
df = pd.read_csv('titanic.csv')
small_df = df[['Age', 'Sex', 'Survived']]
print(small_df.head())
```

Creating a Column

We often want our data in a slightly different format than it originally comes in. For example, our data has the sex of the passenger as a string ("male" or "female"). This is easy for a human to read, but when we do computations on our data later on, we’ll want it as boolean values (Trues and Falses).

We can easily create a new column in our DataFrame that is True if the passenger is male and False if they’re female.

Recall the syntax for selecting the Sex column:
df['Sex']

We create a Pandas Series that will be a series of Trues and Falses (True if the passenger is male and False if the passenger is female).
df['Sex'] == 'male'
Try It Yourself

Now we want to create a column with this result. To create a new column, we use the same bracket syntax (df['male']) and then assign this new value to it.
df['male'] = df['Sex'] == 'male'

Run this code to see the results:
import pandas as pd
df = pd.read_csv('https://sololearn.com/uploads/files/titanic.csv')
df['male'] = df['Sex'] == 'male'
print(df.head())

```{python}
import pandas as pd
df = pd.read_csv('titanic.csv')
df['male'] = df['Sex'] == 'male'
print(df.head())
```

Converting from a Pandas Series to a Numpy Array

We often start with our data in a Pandas DataFrame, but then want to convert it to a numpy array. The values attribute does this for us.

Let's convert the Fare column to a numpy array.

First we recall that we can use the single bracket notation to get a pandas Series of the Fare column as follows.
df['Fare']

Then we use the values attribute to get the values as a numpy array.
df['Fare'].values

Run this code to see the results:
import pandas as pd
df = pd.read_csv('https://sololearn.com/uploads/files/titanic.csv')
print(df['Fare'].values)

```{python}
import pandas as pd
df = pd.read_csv('titanic.csv')
print(df['Fare'].values)
```

Converting from a Pandas DataFrame to a Numpy Array

If we have a pandas DataFrame (instead of a Series as in the last part), we can still use the values attribute, but it returns a 2-dimensional numpy array.

Recall that we can create a smaller pandas DataFrame with the following syntax.
df[['Pclass', 'Fare', 'Age']]

Again, we apply the values attribute to get a numpy array.
df[['Pclass', 'Fare', 'Age']].values 

Run this code to see the results:
import pandas as pd
df = pd.read_csv('https://sololearn.com/uploads/files/titanic.csv')
print(df[['Pclass', 'Fare', 'Age']].values)

```{python}
import pandas as pd
df = pd.read_csv('titanic.csv')
print(df[['Pclass', 'Fare', 'Age']].values)
```

Numpy Shape Attribute

We use the numpy shape attribute to determine the size of our numpy array. The size tells us how many rows and columns are in our data.

First, let's create a numpy array with the Pclass, Fare, and Age.
arr = df[['Pclass', 'Fare', 'Age']].values

If we look at the shape, we get the number of rows and the number of columns:
print(arr.shape) #(887, 3)

Run this code to see the results:
import pandas as pd
df = pd.read_csv('https://sololearn.com/uploads/files/titanic.csv')
arr = df[['Pclass', 'Fare', 'Age']].values
print(arr.shape)

```{python}
import pandas as pd
df = pd.read_csv('titanic.csv')
arr = df[['Pclass', 'Fare', 'Age']].values
print(arr.shape)
```

Select from a Numpy Array

Let's assume we have created the following numpy array:
arr = df[['Pclass', 'Fare', 'Age']].values
Try It Yourself

We can select a single element from a numpy array with the following:
arr[0, 1]
Try It Yourself

This will be the 2nd column of the 1st row (remember that we start counting at 0).
So it'll be the Fare of the 1st passenger, or 7.25.

We can also select a single row, for example, the whole row of the first passenger:
print(arr[0])
Try It Yourself

To select a single column (in this case the Age column), we have to use some special syntax:
print(arr[:,2])
Try It Yourself

Run this code to see the results:
import pandas as pd
df = pd.read_csv('https://sololearn.com/uploads/files/titanic.csv')
arr = df[['Pclass', 'Fare', 'Age']].values
print(arr[0, 1])
print(arr[0])
print(arr[:,2])

```{python}
import pandas as pd
df = pd.read_csv('titanic.csv')
arr = df[['Pclass', 'Fare', 'Age']].values
print(arr[0, 1])
print(arr[0])
print(arr[:,2])
```

Masking

Often times you want to select all the rows that meet a certain criteria.

In this example, we'll select all the rows of children (passengers under the age of 18). A reminder of our definition of the array:
arr = df[['Pclass', 'Fare', 'Age']].values

Recall that we can get the Age column with the following syntax:
arr[:, 2]


We create what we call a mask first. This is an array of boolean values (True/False) of whether the passenger is a child or not.
mask = arr[:, 2] < 18

Let's look at the mask array to make sure we understand it.
array([False, False, False, False, False, False, False, True, False, …

The False values mean adult and the True values mean child, so the first 7 passengers are adults, then 8th is a child, and the 9th is an adult.
Now we use our mask to select just the rows we care about:
arr[mask]

Let's look at this new array.
array([[3., 21.075, 2.],
       [2., 30.0708, 14.],
       [3., 16.7, 4.],
       [3., 7.8542, 14.],

If we recall that the third column is the passengers age, we see that all the rows here are for passengers that are children.

Generally, we don't need to define the mask variable and can do the above in just a single line:
arr[arr[:, 2] < 18] 

Run this code to see the results:
import pandas as pd

df = pd.read_csv('https://sololearn.com/uploads/files/titanic.csv')
# take first 10 values for simplicity
arr = df[['Pclass', 'Fare', 'Age']].values[:10]

mask = arr[:, 2] < 18
print(arr[mask])
print(arr[arr[:, 2] < 18])

```{python}
import pandas as pd

df = pd.read_csv('titanic.csv')
# take first 10 values for simplicity
arr = df[['Pclass', 'Fare', 'Age']].values[:10]

mask = arr[:, 2] < 18
print(arr[mask])
print(arr[arr[:, 2] < 18])
```

Summing and Counting

Let’s say we want to know how many of our passengers are children. We still have the same array definition and can take our mask or boolean values from the previous part.
arr = df[['Pclass', 'Fare', 'Age']].values
mask = arr[:, 2] < 18

Recall that True values are interpreted as 1 and False values are interpreted as 0. So we can just sum up the array and that’s equivalent to counting the number of true values.
print(mask.sum()) 

Again, we don’t need to define the mask variable.
print((arr[:, 2] < 18).sum())

Run this code to see the results:
import pandas as pd

df = pd.read_csv('https://sololearn.com/uploads/files/titanic.csv')
arr = df[['Pclass', 'Fare', 'Age']].values
mask = arr[:, 2] < 18

print(mask.sum())
print((arr[:, 2] < 18).sum())

```{python}
import pandas as pd

df = pd.read_csv('titanic.csv')
arr = df[['Pclass', 'Fare', 'Age']].values
mask = arr[:, 2] < 18

print(mask.sum())
print((arr[:, 2] < 18).sum())
```

Scatter Plot

We can use the matplotlib library to plot our data. Plotting the data can often help us build intuition about our data.

We first need to import matplotlib. It’s standard practice to nickname it plt.
import matplotlib.pyplot as plt

We use the scatter function to plot our data. The first argument of the scatter function is the x-axis (horizontal direction) and the second argument is the y-axis (vertical direction).
plt.scatter(df['Age'], df['Fare'])


This plots the Age on the x-axis and the Fare on the y-axis.

To make it easier to interpret, we can add x and y labels.
plt.xlabel('Age')
plt.ylabel('Fare')


We can also use our data to color code our scatter plot. This will give each of the 3 classes a different color. We add the c parameter and give it a Pandas series. In this case, our Pandas series has 3 possible values (1st, 2nd, and 3rd class), so we'll see our datapoints each get one of three colors.
plt.scatter(df['Age'], df['Fare'], c=df['Pclass'])


The purple dots are first class, the green dots are second class, and the yellow dots are third class.

```{python}
import matplotlib.pyplot as plt
plt.scatter(df['Age'], df['Fare'], c=df['Pclass'])
plt.xlabel('Age')
plt.ylabel('Fare')
plt.show()

```

Line

Now that we can put individual datapoints on a plot, let's see how to draw the line. The plot function does just that. The following draws a line to approximately separate the 1st class from the 2nd and 3rd class. From eyeballing, we’ll put the line from (0, 85) to (80, 5). Our syntax below has a list of the x values and a list of the y values.
plt.plot([0, 80], [85, 5])

```{python}
plt.plot([0, 80], [85, 5])
plt.show()

```

Prep Data with Pandas

Before we can use sklearn to build a model, we need to prep the data with Pandas. Let’s go back to our full dataset and review the Pandas commands.

Here’s a Pandas DataFrame of data with all the columns:


First, we need to make all our columns numerical. Recall how to create the boolean column for Sex.
df['male'] = df['Sex'] == 'male'

Now, let’s take all the features and create a numpy array called X. We first select all the columns we are interested in and then use the values method to convert it to a numpy array.
X = df[['Pclass', 'male', 'Age', 'Siblings/Spouses', 'Parents/Children', 'Fare']].values

Now let’s take the target (the Survived column) and store it in a variable y.
y = df['Survived'].values

Run this code to see the results:
import pandas as pd
df = pd.read_csv('https://sololearn.com/uploads/files/titanic.csv')
df['male'] = df['Sex'] == 'male'
X = df[['Pclass', 'male', 'Age', 'Siblings/Spouses', 'Parents/Children', 'Fare']].values
y = df['Survived'].values
print(X)
print(y)

```{python}
import pandas as pd
df = pd.read_csv('titanic.csv')
df['male'] = df['Sex'] == 'male'
X = df[['Pclass', 'male', 'Age', 'Siblings/Spouses', 'Parents/Children', 'Fare']].values
y = df['Survived'].values
print(X)
print(y)

```

Build a Logistic Regression Model with Sklearn

We start by importing the Logistic Regression model:
from sklearn.linear_model import LogisticRegression 

All sklearn models are built as Python classes. We first instantiate the class.
model = LogisticRegression()

Now we can use our data that we previously prepared to train the model. The fit method is used for building the model. It takes two arguments: X (the features as a 2d numpy array) and y (the target as a 1d numpy array).

For simplicity, let’s first assume that we’re building a Logistic Regression model using just the Fare and Age columns. First we define X to be the feature matrix and y the target array.
X = df[['Fare', 'Age']].values
y = df['Survived'].values

Now we use the fit method to build the model.
model.fit(X, y)

Fitting the model means using the data to choose a line of best fit. We can see the coefficients with the coef_ and intercept_ attributes.
print(model.coef_, model.intercept_)

Run this code to see the results:
import pandas as pd
from sklearn.linear_model import LogisticRegression

df = pd.read_csv('https://sololearn.com/uploads/files/titanic.csv')
X = df[['Fare', 'Age']].values
y = df['Survived'].values

model = LogisticRegression()
model.fit(X, y)

print(model.coef_, model.intercept_)
# [[ 0.01615949 -0.01549065]] [-0.51037152]

```{python}
import pandas as pd
from sklearn.linear_model import LogisticRegression

df = pd.read_csv('titanic.csv')
X = df[['Fare', 'Age']].values
y = df['Survived'].values

model = LogisticRegression()
model.fit(X, y)

print(model.coef_, model.intercept_)

```

Make Predictions with the Model

We really handicapped our model by only using two of the features in the previous parts, so let’s rebuild the model with all of them.
X = df[['Pclass', 'male', 'Age', 'Siblings/Spouses', 'Parents/Children', 'Fare']].values
y = df['Survived'].values
model = LogisticRegression()
model.fit(X, y)

Now we can use the predict method to make predictions.
model.predict(X)

The first passenger in the dataset is:
[3, True, 22.0, 1, 0, 7.25]

This means the passenger is in Pclass 3, are male, are 22 years old, have 1 sibling/spouse aboard, 0 parents/child aboard, and paid $7.25. Let’s see what the model predicts for this passenger. Note that even with one datapoint, the predict method takes a 2-dimensional numpy array and returns a 1-dimensional numpy array.
print(model.predict([[3, True, 22.0, 1, 0, 7.25]])) 
# [0]

The result is 0, which means the model predicts that this passenger did not survive.

Let’s see what the model predicts for the first 5 rows of data and compare it to our target array. We get the first 5 rows of data with X[:5] and the first 5 values of the target with y[:5].
print(model.predict(X[:5])) 
# [0 1 1 1 0]
print(y[:5]) 
# [0 1 1 1 0]

Run this code to see the results:
import pandas as pd
from sklearn.linear_model import LogisticRegression

df = pd.read_csv('https://sololearn.com/uploads/files/titanic.csv')
df['male'] = df['Sex'] == 'male'
X = df[['Pclass', 'male', 'Age', 'Siblings/Spouses', 'Parents/Children', 'Fare']].values
y = df['Survived'].values

model = LogisticRegression()
model.fit(X, y)

print(model.predict([[3, True, 22.0, 1, 0, 7.25]]))
print(model.predict(X[:5]))
print(y[:5])

```{python}
import pandas as pd
from sklearn.linear_model import LogisticRegression

df = pd.read_csv('titanic.csv')
df['male'] = df['Sex'] == 'male'
X = df[['Pclass', 'male', 'Age', 'Siblings/Spouses', 'Parents/Children', 'Fare']].values
y = df['Survived'].values

model = LogisticRegression()
model.fit(X, y)

print(model.predict([[3, True, 22.0, 1, 0, 7.25]]))
print(model.predict(X[:5]))
print(y[:5])
```

Score the Model

We can get a sense of how good our model is by counting the number of datapoints it predicts correctly. This is called the accuracy score.

Let’s create an array that has the predicted y values.
y_pred = model.predict(y)

Now we create an array of boolean values of whether or not our model predicted each passenger correctly.
y == y_pred

To get the number of these that are true, we can use the numpy sum method.
print((y == y_pred).sum())
# 714
Try It Yourself

This means that of the 887 datapoints, the model makes the correct prediction for 714 of them.

To get the percent correct, we divide this by the total number of passengers. We get the total number of passengers using the shape attribute.
y.shape[0]

Thus our accuracy score is computed as follows.
print((y == y_pred).sum() / y.shape[0])
# 0.8038331454340474
Try It Yourself

Thus the model’s accuracy is 80%. In other words, the model makes the correct prediction on 80% of the datapoints.

This is a common enough calculation, that sklearn has already implemented it for us. So we can get the same result by using the score method. The score method uses the model to make a prediction for X and counts what percent of them match y.
print(model.score(X, y))
# 0.8038331454340474
Try It Yourself

With this alternative method of calculating accuracy, we get the same value, 80%.

Run this code to see the results:
import pandas as pd
from sklearn.linear_model import LogisticRegression

df = pd.read_csv('https://sololearn.com/uploads/files/titanic.csv')
df['male'] = df['Sex'] == 'male'
X = df[['Pclass', 'male', 'Age', 'Siblings/Spouses', 'Parents/Children', 'Fare']].values
y = df['Survived'].values

model = LogisticRegression()
model.fit(X, y)

y_pred = model.predict(X)
print((y == y_pred).sum())
print((y == y_pred).sum() / y.shape[0])
print(model.score(X, y))

```{python}
import pandas as pd
from sklearn.linear_model import LogisticRegression

df = pd.read_csv('titanic.csv')
df['male'] = df['Sex'] == 'male'
X = df[['Pclass', 'male', 'Age', 'Siblings/Spouses', 'Parents/Children', 'Fare']].values
y = df['Survived'].values

model = LogisticRegression()
model.fit(X, y)

y_pred = model.predict(X)
print((y == y_pred).sum())
print((y == y_pred).sum() / y.shape[0])
print(model.score(X, y))
```

Introducing the Breast Cancer Dataset

Now that we’ve built up the tools to build a Logistic Regression model for a classification dataset, we’ll introduce a new dataset.

In the breast cancer dataset, each datapoint has measurements from an image of a breast mass and whether or not it’s cancerous. The goal will be to use these measurements to predict if the mass is cancerous.

This dataset is built right into scikit-learn so we won’t need to read in a csv.

Let’s start by loading the dataset and taking a peak at the data and how it’s formatted.
from sklearn.datasets import load_breast_cancer
cancer_data = load_breast_cancer()

The object returned (which we stored in the cancer_data variable) is an object similar to a Python dictionary. We can see the available keys with the keys method.
print(cancer_data.keys())

We’ll start by looking at DESCR, which gives a detailed description of the dataset.
print(cancer_data['DESCR'])

Run this code to see the results:
import pandas as pd
from sklearn.datasets import load_breast_cancer

cancer_data = load_breast_cancer()
print(cancer_data.keys())
print(cancer_data['DESCR'])

```{python}
import pandas as pd
from sklearn.datasets import load_breast_cancer

cancer_data = load_breast_cancer()
print(cancer_data.keys())
print(cancer_data['DESCR'])
```

Loading the Data into Pandas

Let’s pull the feature and target data out of the cancer_data object.

First, the feature data is stored with the 'data' key. When we look at it, we see that it’s a numpy array with 569 rows and 30 columns. That’s because we have 569 datapoints and 30 features.

The following is a numpy array of the data.
cancer_data['data']

We use the shape to see that it is an array with 569 rows and 30 columns.
cancer_data['data'].shape
Try It Yourself

In order to put this in a Pandas DataFrame and make it more human readable, we want the column names. These are stored with the 'feature_names' key.
cancer_data['feature_names']
Try It Yourself

Now we can create a Pandas DataFrame with all our feature data.
df = pd.DataFrame(cancer_data['data'], columns=cancer_data['feature_names'])
print(df.head())
Try It Yourself

Result:


We can see that we have 30 columns in the DataFrame, since we have 30 features. The output is truncated so that it’ll fit on the screen. We used the head method, so our result only has 5 datapoints.

We still need to put the target data in our DataFrame, which can be found with the 'target' key. We can see that the target is a 1-dimensional numpy array of 1’s and 0’s.
cancer_data['target']
Try It Yourself

If we look at the shape of the array, we see that it’s a 1-dimensional array with 569 values (which was how many datapoints we had).
cancer_data['target'].shape
Try It Yourself

In order to interpret these 1’s and 0’s, we need to know whether 1 or 0 is benign or malignant. This is given by the target_names
cancer_data['target_names']
Try It Yourself

This gives the array ['malignant' 'benign'] which tells us that 0 means malignant and 1 means benign. Let’s add this data to the Pandas DataFrame.
df['target'] = cancer_data['target']
df.head()
Try It Yourself

Run this code to see the results:
import pandas as pd
from sklearn.datasets import load_breast_cancer

cancer_data = load_breast_cancer()

df = pd.DataFrame(cancer_data['data'], columns=cancer_data['feature_names'])
df['target'] = cancer_data['target']
print(df.head())

```{python}
cancer_data['data']
cancer_data['data'].shape
cancer_data['feature_names']
df = pd.DataFrame(cancer_data['data'],columns=cancer_data['feature_names'])
print(df.head())
cancer_data['target']
cancer_data['target'].shape
cancer_data['target_names']
df['target'] = cancer_data['target']
df.head()
```

```{python}
import pandas as pd
from sklearn.datasets import load_breast_cancer

cancer_data = load_breast_cancer()

df = pd.DataFrame(cancer_data['data'], columns=cancer_data['feature_names'])
df['target'] = cancer_data['target']
print(df.head())
```

Build a Logistic Regression Model

Now that we’ve taken a look at our data and gotten it into a comfortable format, we can build our feature matrix X and target array y so that we can build a Logistic Regression model.
X = df[cancer_data.feature_names].values
y = df['target'].values

Now we create a Logistic Regression object and use the fit method to build the model.
model = LogisticRegression()
model.fit(X, y)

When we run this code we get a Convergence Warning. This means that the model needs more time to find the optimal solution. One option is to increase the number of iterations. You can also switch to a different solver, which is what we will do. The solver is the algorithm that the model uses to find the equation of the line. You can see the possible solvers in the Logistic Regression documentation.
model = LogisticRegression(solver='liblinear')
model.fit(X, y) 

Let’s see what the model predicts for the first datapoint in our dataset. Recall that the predict method takes a 2-dimensional array so we must put the datapoint in a list.
model.predict([X[0]])

So the model predicts that the first datapoint is benign.

To see how well the model performs over the whole dataset, we use the score method to see the accuracy of the model.
model.score(X, y)

Run this code to see the results:
import pandas as pd
from sklearn.datasets import load_breast_cancer
from sklearn.linear_model import LogisticRegression

cancer_data = load_breast_cancer()
df = pd.DataFrame(cancer_data['data'], columns=cancer_data['feature_names'])
df['target'] = cancer_data['target']

X = df[cancer_data.feature_names].values
y = df['target'].values

model = LogisticRegression(solver='liblinear')
model.fit(X, y)
print("prediction for datapoint 0:", model.predict([X[0]]))
print(model.score(X, y))

```{python}
import pandas as pd
from sklearn.datasets import load_breast_cancer
from sklearn.linear_model import LogisticRegression

cancer_data = load_breast_cancer()
df = pd.DataFrame(cancer_data['data'],columns=cancer_data['feature_names'])
df['target'] = cancer_data['target']

X = df[cancer_data.feature_names].values
y = df['target'].values

model = LogisticRegression(solver='liblinear')
model.fit(X, y)
print("prediction for datapoint 0:", model.predict([X[0]]))
print(model.score(X, y))

```

Accuracy, Precision, Recall & F1 Score in Sklearn

Scikit-learn has a function built in for each of the metrics that we have introduced. We have a separate function for each of the accuracy, precision, recall and F1 score.

In order to use them, let’s start by recalling our code from the previous module to build a Logistic Regression model. The code reads in the Titanic dataset from the csv file and puts it in a Pandas DataFrame. Then we create a feature matrix X and target values y. We create a Logistic Regression model and fit it to our dataset. Finally, we create a variable y_pred of our predictions.
df = pd.read_csv('https://sololearn.com/uploads/files/titanic.csv')
df['male'] = df['Sex'] == 'male'
X = df[['Pclass', 'male', 'Age', 'Siblings/Spouses', 'Parents/Children', 'Fare']].values
y = df['Survived'].values
model = LogisticRegression()
model.fit(X, y)
y_pred = model.predict(X)

Now we’re ready to use our metric functions. Let’s import them from scikit-learn.
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

Each function takes two 1-dimensional numpy arrays: the true values of the target & the predicted values of the target. We have the true values of the target and the predicted values of the target. Thus we can use the metric functions as follows.
print("accuracy:", accuracy_score(y, y_pred))
print("precision:", precision_score(y, y_pred))
print("recall:", recall_score(y, y_pred))
print("f1 score:", f1_score(y, y_pred))

```{python}
df = pd.read_csv('titanic.csv')
df['male'] = df['Sex'] == 'male'
X = df[['Pclass', 'male', 'Age', 'Siblings/Spouses','Parents/Children', 'Fare']].values
y = df['Survived'].values
model = LogisticRegression()
model.fit(X, y)
y_pred = model.predict(X)

from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

print("accuracy:", accuracy_score(y, y_pred))
print("precision:", precision_score(y, y_pred))
print("recall:", recall_score(y, y_pred))
print("f1 score:", f1_score(y, y_pred))

```

Confusion Matrix in Sklearn

Scikit-learn has a confusion matrix function that we can use to get the four values in the confusion matrix (true positives, false positives, false negatives, and true negatives). Assuming y is our true target values and y_pred is the predicted values, we can use the confusion_matrix function as follows:
from sklearn.metrics import confusion_matrix
print(confusion_matrix(y, y_pred))
# Output:
# [[475  70]
#  [103 239]]

```{python}
from sklearn.metrics import confusion_matrix
print(confusion_matrix(y, y_pred))
```

Training and Testing in Sklearn

Scikit-learn has a function built in for splitting the data into a training set and a test set.

Assuming we have a 2-dimensional numpy array X of our features and a 1-dimensional numpy array y of the target, we can use the train_test_split function. It will randomly put each datapoint in either the training set or the test set. By default the training set is 75% of the data and the test set is the remaining 25% of the data.
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y)

Let’s use the shape attribute to see the sizes of our datasets.
print("whole dataset:", X.shape, y.shape)
print("training set:", X_train.shape, y_train.shape)
print("test set:", X_test.shape, y_test.shape)


Run this code to see the results:
import pandas as pd
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split

df = pd.read_csv('https://sololearn.com/uploads/files/titanic.csv')
df['male'] = df['Sex'] == 'male'
X = df[['Pclass', 'male', 'Age', 'Siblings/Spouses', 'Parents/Children', 'Fare']].values
y = df['Survived'].values

X_train, X_test, y_train, y_test = train_test_split(X, y)

print("whole dataset:", X.shape, y.shape)
print("training set:", X_train.shape, y_train.shape)
print("test set:", X_test.shape, y_test.shape)

```{python}
import pandas as pd
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split

df = pd.read_csv('titanic.csv')
df['male'] = df['Sex'] == 'male'
X = df[['Pclass', 'male', 'Age', 'Siblings/Spouses', 'Parents/Children', 'Fare']].values
y = df['Survived'].values

X_train, X_test, y_train, y_test = train_test_split(X, y)

print("whole dataset:", X.shape, y.shape)
print("training set:", X_train.shape, y_train.shape)
print("test set:", X_test.shape, y_test.shape)
```

Building a Scikit-learn Model Using a Training Set

Now that we know how to split our data into a training set and a test set, we need to modify how we build and evaluate the model. All of the model building is done with the training set and all of the evaluation is done with the test set.

In the last module, we built a model and evaluated it on the same dataset. Now we build the model using the training set.
model = LogisticRegression()
model.fit(X_train, y_train)

And we evaluate the model using the test set.
print(model.score(X_test, y_test))

In fact, all of the metrics we calculate in the previous parts should be calculated on the test set.
y_pred = model.predict(X_test)
print("accuracy:", accuracy_score(y_test, y_pred))
print("precision:", precision_score(y_test, y_pred))
print("recall:", recall_score(y_test, y_pred))
print("f1 score:", f1_score(y_test, y_pred))

```{python}
model = LogisticRegression()
model.fit(X_train, y_train)

print(model.score(X_test, y_test))

y_pred = model.predict(X_test)
print("accuracy:", accuracy_score(y_test, y_pred))
print("precision:", precision_score(y_test, y_pred))
print("recall:", recall_score(y_test, y_pred))
print("f1 score:", f1_score(y_test, y_pred))
```

Using a Random State

As we noticed in the previous part, when we randomly split the data into a training set and a test set, we end up with different datapoints in each set each time we run the code. This is a result of randomness, and we need it to be random for it to be effective, but this can sometimes make it difficult to test the code.

For example, each time we run the following code, we will get different results.
from sklearn.model_selection import train_test_split

X = [[1, 1], [2, 2], [3, 3], [4, 4]]
y = [0, 0, 1, 1]

X_train, X_test, y_train, y_test = train_test_split(X, y)
print('X_train', X_train)
print('X_test', X_test)

```{python}
from sklearn.model_selection import train_test_split

X = [[1, 1], [2, 2], [3, 3], [4, 4]]
y = [0, 0, 1, 1]

X_train, X_test, y_train, y_test = train_test_split(X, y)
print('X_train', X_train)
print('X_test', X_test)
```

To get the same split every time, we can use the random_state attribute. We choose an arbitrary number to give it, and then every time we run the code, we will get the same split.
from sklearn.model_selection import train_test_split

X = [[1, 1], [2, 2], [3, 3], [4, 4]]
y = [0, 0, 1, 1]

X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=27)
print('X_train', X_train)
print('X_test', X_test)

```{python}
from sklearn.model_selection import train_test_split

X = [[1, 1], [2, 2], [3, 3], [4, 4]]
y = [0, 0, 1, 1]

X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=27)
print('X_train', X_train)
print('X_test', X_test)
```

Precision = TP / (TP + FP) 
  #we want a low amount false positives
Sensitivity = Recall = TP / (TP + FN) 
  #we want a low amount of false negatives
Specificity = TN / (TN + FP)
  #we want to catch the true negatives
Accuracy = (TP + TN) / total Cases 
  #we want a low amount of wrong predictions

Goal: Max Sensitivity and Specificity

Sensitivity & Specificity in Scikit-learn

Scikit-learn has not defined functions for sensitivity and specificity, but we can do it ourselves. Sensitivity is the same as recall, so it is easy to define.
from sklearn.metrics import recall_score
sensitivity_score = recall_score
print(sensitivity_score(y_test, y_pred)) 
# 0.6829268292682927

Now, to define specificity, if we realize that it is also the recall of the negative class, we can get the value from the sklearn function precision_recall_fscore_support.

Let’s look at the output of precision_recall_fscore_support.
from sklearn.metrics import precision_recall_fscore_support
print(precision_recall_fscore_support(y, y_pred))

The second array is the recall, so we can ignore the other three arrays. There are two values. The first is the recall of the negative class and the second is the recall of the positive class. The second value is the standard recall or sensitivity value, and you can see the value matches what we got above. The first value is the specificity. So let’s write a function to get just that value.
def specificity_score(y_true, y_pred):
    p, r, f, s = precision_recall_fscore_support(y_true, y_pred)
    return r[0]
print(specificity_score(y_test, y_pred)) 
# 0.9214285714285714

Note that in the code sample we use a random state in the train test split so that every time you run the code you will get the same results.
import pandas as pd
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import recall_score, precision_recall_fscore_support

sensitivity_score = recall_score
def specificity_score(y_true, y_pred):
    p, r, f, s = precision_recall_fscore_support(y_true, y_pred)
    return r[0]

df = pd.read_csv('https://sololearn.com/uploads/files/titanic.csv')
df['male'] = df['Sex'] == 'male'
X = df[['Pclass', 'male', 'Age', 'Siblings/Spouses', 'Parents/Children', 'Fare']].values
y = df['Survived'].values

X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=5)

model = LogisticRegression()
model.fit(X_train, y_train)
y_pred = model.predict(X_test)

print("sensitivity:", sensitivity_score(y_test, y_pred))
print("specificity:", specificity_score(y_test, y_pred))

```{python}
import pandas as pd
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import recall_score, precision_recall_fscore_support

sensitivity_score = recall_score
def specificity_score(y_true, y_pred):
    p, r, f, s = precision_recall_fscore_support(y_true, y_pred)
    return r[0]

df = pd.read_csv('titanic.csv')
df['male'] = df['Sex'] == 'male'
X = df[['Pclass', 'male', 'Age', 'Siblings/Spouses', 'Parents/Children', 'Fare']].values
y = df['Survived'].values

X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=5)

model = LogisticRegression()
model.fit(X_train, y_train)
y_pred = model.predict(X_test)

print("sensitivity:", sensitivity_score(y_test, y_pred))
print("specificity:", specificity_score(y_test, y_pred))
```

Adjusting the Logistic Regression Threshold in Sklearn

When you use scikit-learn’s predict method, you are given 0 and 1 values of the prediction. However, behind the scenes the Logistic Regression model is getting a probability value between 0 and 1 for each datapoint and then rounding to either 0 or 1. If we want to choose a different threshold besides 0.5, we’ll want those probability values. We can use the predict_proba function to get them.
(model.predict_proba(X_test)

The result is a numpy array with 2 values for each datapoint (e.g. [0.78, 0.22]). You’ll notice that the two values sum to 1. The first value is the probability that the datapoint is in the 0 class (didn’t survive) and the second is the probability that the datapoint is in the 1 class (survived). We only need the second column of this result, which we can pull with the following numpy syntax.
model.predict_proba(X_test)[:, 1]

Now we just want to compare these probability values with our threshold. Say we want a threshold of 0.75. We compare the above array to 0.75. This will give us an array of True/False values which will be our array of predicted target values.
y_pred = model.predict_proba(X_test)[:, 1] > 0.75

A threshold of 0.75 means we need to be more confident in order to make a positive prediction. This results in fewer positive predictions and more negative predictions.

Now we can use any scikit-learn metrics from before using y_test as our true values and y_pred as our predicted values.
print("precision:", precision_score(y_test, y_pred))
print("recall:", recall_score(y_test, y_pred))

Run this code to see the results:
import pandas as pd
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import precision_score, recall_score
from sklearn.model_selection import train_test_split

df = pd.read_csv('https://sololearn.com/uploads/files/titanic.csv')
df['male'] = df['Sex'] == 'male'
X = df[['Pclass', 'male', 'Age', 'Siblings/Spouses', 'Parents/Children', 'Fare']].values
y = df['Survived'].values

X_train, X_test, y_train, y_test = train_test_split(X, y)

model = LogisticRegression()
model.fit(X_train, y_train)

print("predict proba:")
print(model.predict_proba(X_test))

y_pred = model.predict_proba(X_test)[:, 1] > 0.75

print("precision:", precision_score(y_test, y_pred))
print("recall:", recall_score(y_test, y_pred))

```{python}
import pandas as pd
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import precision_score, recall_score
from sklearn.model_selection import train_test_split

df = pd.read_csv('titanic.csv')
df['male'] = df['Sex'] == 'male'
X = df[['Pclass', 'male', 'Age', 'Siblings/Spouses', 'Parents/Children', 'Fare']].values
y = df['Survived'].values

X_train, X_test, y_train, y_test = train_test_split(X, y)

model = LogisticRegression()
model.fit(X_train, y_train)

print("predict proba:")
print(model.predict_proba(X_test))

y_pred = model.predict_proba(X_test)[:, 1] > 0.75

print("precision:", precision_score(y_test, y_pred))
print("recall:", recall_score(y_test, y_pred))
```































