---
title: "SoloLearn Machine Learning"
author: "Ken Harmon"
date: "`r format(Sys.time(), '%Y %B %d')`"
output:
  html_document:
    keep_md: yes
    code_folding: hide
    fig_height: 6
    fig_width: 12
    fig_align: center
  pdf_document: default
editor_options:
  chunk_output_type: console
---
 
https://www.sololearn.com/Play/machine-learning

# {.tabset .tabset-fade}

```{r, echo=FALSE}
library(reticulate)
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE)
knitr::knit_engines$set(python = reticulate::eng_python)
reticulate::repl_python()

```

```{r}
#py_install("graphviz")
#py_install("sqlalchemy")
```

Statistics with Python

We can calculate all of these operations with Python. We will use the Python package numpy. We will use numpy more later for manipulating arrays, but for now we will just use a few functions for statistical calculations: mean, median, percentile, std, var.

First we import the package. It is standard practice to nickname numpy as np.
import numpy as np

Let’s initialize the variable data to have the list of ages.
data = [15, 16, 18, 19, 22, 24, 29, 30, 34]

Now we can use the numpy functions. For the mean, median, standard deviation and variance functions, we just pass in the data list. For the percentile function we pass the data list and the percentile (as a number between 0 and 100).

Try it now:
import numpy as np

data = [15, 16, 18, 19, 22, 24, 29, 30, 34]

print("mean:", np.mean(data))
print("median:", np.median(data))
print("50th percentile (median):", np.percentile(data, 50))
print("25th percentile:", np.percentile(data, 25))
print("75th percentile:", np.percentile(data, 75))
print("standard deviation:", np.std(data))
print("variance:", np.var(data))

```{python}
import numpy as np

data = [15, 16, 18, 19, 22, 24, 29, 30, 34]

print("mean:", np.mean(data))
print("median:", np.median(data))
print("50th percentile (median):", np.percentile(data, 50))
print("25th percentile:", np.percentile(data, 25))
print("75th percentile:", np.percentile(data, 75))
print("standard deviation:", np.std(data))
print("variance:", np.var(data))
```

Read in Your Data

We need to start by importing Pandas. It's standard practice to nickname it pd so that it's faster to type later on.
import pandas as pd 

We will be working with a dataset of Titanic passengers. For each passenger, we’ll have some data on them as well as whether or not they survived the crash.

Our data is stored as CSV (comma-separated values) file. The titanic.csv file is below. The first line is the header and then each subsequent line is the data for a single passenger.
Survived, Pclass, Sex, Age, Siblings/Spouses, Parents/Children, Fare
0, 3, male, 22.0, 1, 0, 7.25
1, 1, female, 38.0, 1, 0, 71.2833
1, 3, female, 26.0, 0, 0, 7.925
1, 1, female, 35.0, 1, 0, 53.1

We're going to pull the data into pandas so we can view it as a DataFrame.

The read_csv function takes a file in csv format and converts it to a Pandas DataFrame.
df = pd.read_csv('titanic.csv')

The object df is now our pandas dataframe with the Titanic dataset. Now we can use the head method to look at the data.
The head method returns the first 5 rows of the DataFrame.
print(df.head())

Run this code to see the results:
import pandas as pd
df = pd.read_csv('https://sololearn.com/uploads/files/titanic.csv')
print(df.head())

```{python}
import pandas as pd 

df = pd.read_csv('titanic.csv')

print(df.head())

```

Summarize the Data

Usually our data is much too big for us to be able to display it all.
Looking at the first few rows is the first step to understanding our data, but then we want to look at some summary statistics.
In pandas, we can use the describe method. It returns a table of statistics about the columns.
print(df.describe())
We add a line in the code below to force python to display all 6 columns. Without the line, it will abbreviate the results.

Run this code to see the results:
import pandas as pd
pd.options.display.max_columns = 6
df = pd.read_csv('https://sololearn.com/uploads/files/titanic.csv')
print(df.describe())

```{python}
import pandas as pd
pd.options.display.max_columns = 6
df = pd.read_csv('titanic.csv')
print(df.describe())
```

Selecting a Single Column

We often will only want to deal with some of the columns that we have in our dataset. To select a single column, we use the square brackets and the column name.

In this example, we're selecting just the column with the passenger fares.
col = df['Fare']
print(col)

The result is what we call a Pandas Series.
A series is like a DataFrame, but it's just a single column.

Run this code to see the results:
import pandas as pd
df = pd.read_csv('https://sololearn.com/uploads/files/titanic.csv')
col = df['Fare']
print(col) 

```{python}
import pandas as pd
df = pd.read_csv('titanic.csv')
col = df['Fare']
print(col) 
```

Selecting Multiple Columns

We can also select multiple columns from our original DataFrame, creating a smaller DataFrame.
We're going to select just the Age, Sex, and Survived columns from our original DataFrame.

We put these values in a list as follows:
['Age', 'Sex', 'Survived']

Now we use that list inside of the bracket notation df[...] When printing a large DataFrame that’s too big to display, you can use the head method to print just the first 5 rows.
small_df = df[['Age', 'Sex', 'Survived']]
print(small_df.head()) 

Run this code to see the results:
import pandas as pd
df = pd.read_csv('https://sololearn.com/uploads/files/titanic.csv')
small_df = df[['Age', 'Sex', 'Survived']]
print(small_df.head())

```{python}
import pandas as pd
df = pd.read_csv('titanic.csv')
small_df = df[['Age', 'Sex', 'Survived']]
print(small_df.head())
```

Creating a Column

We often want our data in a slightly different format than it originally comes in. For example, our data has the sex of the passenger as a string ("male" or "female"). This is easy for a human to read, but when we do computations on our data later on, we’ll want it as boolean values (Trues and Falses).

We can easily create a new column in our DataFrame that is True if the passenger is male and False if they’re female.

Recall the syntax for selecting the Sex column:
df['Sex']

We create a Pandas Series that will be a series of Trues and Falses (True if the passenger is male and False if the passenger is female).
df['Sex'] == 'male'
Try It Yourself

Now we want to create a column with this result. To create a new column, we use the same bracket syntax (df['male']) and then assign this new value to it.
df['male'] = df['Sex'] == 'male'

Run this code to see the results:
import pandas as pd
df = pd.read_csv('https://sololearn.com/uploads/files/titanic.csv')
df['male'] = df['Sex'] == 'male'
print(df.head())

```{python}
import pandas as pd
df = pd.read_csv('titanic.csv')
df['male'] = df['Sex'] == 'male'
print(df.head())
```

Converting from a Pandas Series to a Numpy Array

We often start with our data in a Pandas DataFrame, but then want to convert it to a numpy array. The values attribute does this for us.

Let's convert the Fare column to a numpy array.

First we recall that we can use the single bracket notation to get a pandas Series of the Fare column as follows.
df['Fare']

Then we use the values attribute to get the values as a numpy array.
df['Fare'].values

Run this code to see the results:
import pandas as pd
df = pd.read_csv('https://sololearn.com/uploads/files/titanic.csv')
print(df['Fare'].values)

```{python}
import pandas as pd
df = pd.read_csv('titanic.csv')
print(df['Fare'].values)
```

Converting from a Pandas DataFrame to a Numpy Array

If we have a pandas DataFrame (instead of a Series as in the last part), we can still use the values attribute, but it returns a 2-dimensional numpy array.

Recall that we can create a smaller pandas DataFrame with the following syntax.
df[['Pclass', 'Fare', 'Age']]

Again, we apply the values attribute to get a numpy array.
df[['Pclass', 'Fare', 'Age']].values 

Run this code to see the results:
import pandas as pd
df = pd.read_csv('https://sololearn.com/uploads/files/titanic.csv')
print(df[['Pclass', 'Fare', 'Age']].values)

```{python}
import pandas as pd
df = pd.read_csv('titanic.csv')
print(df[['Pclass', 'Fare', 'Age']].values)
```

Numpy Shape Attribute

We use the numpy shape attribute to determine the size of our numpy array. The size tells us how many rows and columns are in our data.

First, let's create a numpy array with the Pclass, Fare, and Age.
arr = df[['Pclass', 'Fare', 'Age']].values

If we look at the shape, we get the number of rows and the number of columns:
print(arr.shape) #(887, 3)

Run this code to see the results:
import pandas as pd
df = pd.read_csv('https://sololearn.com/uploads/files/titanic.csv')
arr = df[['Pclass', 'Fare', 'Age']].values
print(arr.shape)

```{python}
import pandas as pd
df = pd.read_csv('titanic.csv')
arr = df[['Pclass', 'Fare', 'Age']].values
print(arr.shape)
```

Select from a Numpy Array

Let's assume we have created the following numpy array:
arr = df[['Pclass', 'Fare', 'Age']].values
Try It Yourself

We can select a single element from a numpy array with the following:
arr[0, 1]
Try It Yourself

This will be the 2nd column of the 1st row (remember that we start counting at 0).
So it'll be the Fare of the 1st passenger, or 7.25.

We can also select a single row, for example, the whole row of the first passenger:
print(arr[0])
Try It Yourself

To select a single column (in this case the Age column), we have to use some special syntax:
print(arr[:,2])
Try It Yourself

Run this code to see the results:
import pandas as pd
df = pd.read_csv('https://sololearn.com/uploads/files/titanic.csv')
arr = df[['Pclass', 'Fare', 'Age']].values
print(arr[0, 1])
print(arr[0])
print(arr[:,2])

```{python}
import pandas as pd
df = pd.read_csv('titanic.csv')
arr = df[['Pclass', 'Fare', 'Age']].values
print(arr[0, 1])
print(arr[0])
print(arr[:,2])
```

Masking

Often times you want to select all the rows that meet a certain criteria.

In this example, we'll select all the rows of children (passengers under the age of 18). A reminder of our definition of the array:
arr = df[['Pclass', 'Fare', 'Age']].values

Recall that we can get the Age column with the following syntax:
arr[:, 2]


We create what we call a mask first. This is an array of boolean values (True/False) of whether the passenger is a child or not.
mask = arr[:, 2] < 18

Let's look at the mask array to make sure we understand it.
array([False, False, False, False, False, False, False, True, False, …

The False values mean adult and the True values mean child, so the first 7 passengers are adults, then 8th is a child, and the 9th is an adult.
Now we use our mask to select just the rows we care about:
arr[mask]

Let's look at this new array.
array([[3., 21.075, 2.],
       [2., 30.0708, 14.],
       [3., 16.7, 4.],
       [3., 7.8542, 14.],

If we recall that the third column is the passengers age, we see that all the rows here are for passengers that are children.

Generally, we don't need to define the mask variable and can do the above in just a single line:
arr[arr[:, 2] < 18] 

Run this code to see the results:
import pandas as pd

df = pd.read_csv('https://sololearn.com/uploads/files/titanic.csv')
# take first 10 values for simplicity
arr = df[['Pclass', 'Fare', 'Age']].values[:10]

mask = arr[:, 2] < 18
print(arr[mask])
print(arr[arr[:, 2] < 18])

```{python}
import pandas as pd

df = pd.read_csv('titanic.csv')
# take first 10 values for simplicity
arr = df[['Pclass', 'Fare', 'Age']].values[:10]

mask = arr[:, 2] < 18
print(arr[mask])
print(arr[arr[:, 2] < 18])
```

Summing and Counting

Let’s say we want to know how many of our passengers are children. We still have the same array definition and can take our mask or boolean values from the previous part.
arr = df[['Pclass', 'Fare', 'Age']].values
mask = arr[:, 2] < 18

Recall that True values are interpreted as 1 and False values are interpreted as 0. So we can just sum up the array and that’s equivalent to counting the number of true values.
print(mask.sum()) 

Again, we don’t need to define the mask variable.
print((arr[:, 2] < 18).sum())

Run this code to see the results:
import pandas as pd

df = pd.read_csv('https://sololearn.com/uploads/files/titanic.csv')
arr = df[['Pclass', 'Fare', 'Age']].values
mask = arr[:, 2] < 18

print(mask.sum())
print((arr[:, 2] < 18).sum())

```{python}
import pandas as pd

df = pd.read_csv('titanic.csv')
arr = df[['Pclass', 'Fare', 'Age']].values
mask = arr[:, 2] < 18

print(mask.sum())
print((arr[:, 2] < 18).sum())
```

Scatter Plot

We can use the matplotlib library to plot our data. Plotting the data can often help us build intuition about our data.

We first need to import matplotlib. It’s standard practice to nickname it plt.
import matplotlib.pyplot as plt

We use the scatter function to plot our data. The first argument of the scatter function is the x-axis (horizontal direction) and the second argument is the y-axis (vertical direction).
plt.scatter(df['Age'], df['Fare'])


This plots the Age on the x-axis and the Fare on the y-axis.

To make it easier to interpret, we can add x and y labels.
plt.xlabel('Age')
plt.ylabel('Fare')


We can also use our data to color code our scatter plot. This will give each of the 3 classes a different color. We add the c parameter and give it a Pandas series. In this case, our Pandas series has 3 possible values (1st, 2nd, and 3rd class), so we'll see our datapoints each get one of three colors.
plt.scatter(df['Age'], df['Fare'], c=df['Pclass'])


The purple dots are first class, the green dots are second class, and the yellow dots are third class.

```{python}
import matplotlib.pyplot as plt
plt.scatter(df['Age'], df['Fare'], c=df['Pclass'])
plt.xlabel('Age')
plt.ylabel('Fare')
plt.show()

```

Line

Now that we can put individual datapoints on a plot, let's see how to draw the line. The plot function does just that. The following draws a line to approximately separate the 1st class from the 2nd and 3rd class. From eyeballing, we’ll put the line from (0, 85) to (80, 5). Our syntax below has a list of the x values and a list of the y values.
plt.plot([0, 80], [85, 5])

```{python}
plt.plot([0, 80], [85, 5])
plt.show()

```

Prep Data with Pandas

Before we can use sklearn to build a model, we need to prep the data with Pandas. Let’s go back to our full dataset and review the Pandas commands.

Here’s a Pandas DataFrame of data with all the columns:


First, we need to make all our columns numerical. Recall how to create the boolean column for Sex.
df['male'] = df['Sex'] == 'male'

Now, let’s take all the features and create a numpy array called X. We first select all the columns we are interested in and then use the values method to convert it to a numpy array.
X = df[['Pclass', 'male', 'Age', 'Siblings/Spouses', 'Parents/Children', 'Fare']].values

Now let’s take the target (the Survived column) and store it in a variable y.
y = df['Survived'].values

Run this code to see the results:
import pandas as pd
df = pd.read_csv('https://sololearn.com/uploads/files/titanic.csv')
df['male'] = df['Sex'] == 'male'
X = df[['Pclass', 'male', 'Age', 'Siblings/Spouses', 'Parents/Children', 'Fare']].values
y = df['Survived'].values
print(X)
print(y)

```{python}
import pandas as pd
df = pd.read_csv('titanic.csv')
df['male'] = df['Sex'] == 'male'
X = df[['Pclass', 'male', 'Age', 'Siblings/Spouses', 'Parents/Children', 'Fare']].values
y = df['Survived'].values
print(X)
print(y)

```

Build a Logistic Regression Model with Sklearn

We start by importing the Logistic Regression model:
from sklearn.linear_model import LogisticRegression 

All sklearn models are built as Python classes. We first instantiate the class.
model = LogisticRegression()

Now we can use our data that we previously prepared to train the model. The fit method is used for building the model. It takes two arguments: X (the features as a 2d numpy array) and y (the target as a 1d numpy array).

For simplicity, let’s first assume that we’re building a Logistic Regression model using just the Fare and Age columns. First we define X to be the feature matrix and y the target array.
X = df[['Fare', 'Age']].values
y = df['Survived'].values

Now we use the fit method to build the model.
model.fit(X, y)

Fitting the model means using the data to choose a line of best fit. We can see the coefficients with the coef_ and intercept_ attributes.
print(model.coef_, model.intercept_)

Run this code to see the results:
import pandas as pd
from sklearn.linear_model import LogisticRegression

df = pd.read_csv('https://sololearn.com/uploads/files/titanic.csv')
X = df[['Fare', 'Age']].values
y = df['Survived'].values

model = LogisticRegression()
model.fit(X, y)

print(model.coef_, model.intercept_)
# [[ 0.01615949 -0.01549065]] [-0.51037152]

```{python}
import pandas as pd
from sklearn.linear_model import LogisticRegression

df = pd.read_csv('titanic.csv')
X = df[['Fare', 'Age']].values
y = df['Survived'].values

model = LogisticRegression()
model.fit(X, y)

print(model.coef_, model.intercept_)

```

Make Predictions with the Model

We really handicapped our model by only using two of the features in the previous parts, so let’s rebuild the model with all of them.
X = df[['Pclass', 'male', 'Age', 'Siblings/Spouses', 'Parents/Children', 'Fare']].values
y = df['Survived'].values
model = LogisticRegression()
model.fit(X, y)

Now we can use the predict method to make predictions.
model.predict(X)

The first passenger in the dataset is:
[3, True, 22.0, 1, 0, 7.25]

This means the passenger is in Pclass 3, are male, are 22 years old, have 1 sibling/spouse aboard, 0 parents/child aboard, and paid $7.25. Let’s see what the model predicts for this passenger. Note that even with one datapoint, the predict method takes a 2-dimensional numpy array and returns a 1-dimensional numpy array.
print(model.predict([[3, True, 22.0, 1, 0, 7.25]])) 
# [0]

The result is 0, which means the model predicts that this passenger did not survive.

Let’s see what the model predicts for the first 5 rows of data and compare it to our target array. We get the first 5 rows of data with X[:5] and the first 5 values of the target with y[:5].
print(model.predict(X[:5])) 
# [0 1 1 1 0]
print(y[:5]) 
# [0 1 1 1 0]

Run this code to see the results:
import pandas as pd
from sklearn.linear_model import LogisticRegression

df = pd.read_csv('https://sololearn.com/uploads/files/titanic.csv')
df['male'] = df['Sex'] == 'male'
X = df[['Pclass', 'male', 'Age', 'Siblings/Spouses', 'Parents/Children', 'Fare']].values
y = df['Survived'].values

model = LogisticRegression()
model.fit(X, y)

print(model.predict([[3, True, 22.0, 1, 0, 7.25]]))
print(model.predict(X[:5]))
print(y[:5])

```{python}
import pandas as pd
from sklearn.linear_model import LogisticRegression

df = pd.read_csv('titanic.csv')
df['male'] = df['Sex'] == 'male'
X = df[['Pclass', 'male', 'Age', 'Siblings/Spouses', 'Parents/Children', 'Fare']].values
y = df['Survived'].values

model = LogisticRegression()
model.fit(X, y)

print(model.predict([[3, True, 22.0, 1, 0, 7.25]]))
print(model.predict(X[:5]))
print(y[:5])
```

Score the Model

We can get a sense of how good our model is by counting the number of datapoints it predicts correctly. This is called the accuracy score.

Let’s create an array that has the predicted y values.
y_pred = model.predict(y)

Now we create an array of boolean values of whether or not our model predicted each passenger correctly.
y == y_pred

To get the number of these that are true, we can use the numpy sum method.
print((y == y_pred).sum())
# 714
Try It Yourself

This means that of the 887 datapoints, the model makes the correct prediction for 714 of them.

To get the percent correct, we divide this by the total number of passengers. We get the total number of passengers using the shape attribute.
y.shape[0]

Thus our accuracy score is computed as follows.
print((y == y_pred).sum() / y.shape[0])
# 0.8038331454340474
Try It Yourself

Thus the model’s accuracy is 80%. In other words, the model makes the correct prediction on 80% of the datapoints.

This is a common enough calculation, that sklearn has already implemented it for us. So we can get the same result by using the score method. The score method uses the model to make a prediction for X and counts what percent of them match y.
print(model.score(X, y))
# 0.8038331454340474
Try It Yourself

With this alternative method of calculating accuracy, we get the same value, 80%.

Run this code to see the results:
import pandas as pd
from sklearn.linear_model import LogisticRegression

df = pd.read_csv('https://sololearn.com/uploads/files/titanic.csv')
df['male'] = df['Sex'] == 'male'
X = df[['Pclass', 'male', 'Age', 'Siblings/Spouses', 'Parents/Children', 'Fare']].values
y = df['Survived'].values

model = LogisticRegression()
model.fit(X, y)

y_pred = model.predict(X)
print((y == y_pred).sum())
print((y == y_pred).sum() / y.shape[0])
print(model.score(X, y))

```{python}
import pandas as pd
from sklearn.linear_model import LogisticRegression

df = pd.read_csv('titanic.csv')
df['male'] = df['Sex'] == 'male'
X = df[['Pclass', 'male', 'Age', 'Siblings/Spouses', 'Parents/Children', 'Fare']].values
y = df['Survived'].values

model = LogisticRegression()
model.fit(X, y)

y_pred = model.predict(X)
print((y == y_pred).sum())
print((y == y_pred).sum() / y.shape[0])
print(model.score(X, y))
```

Introducing the Breast Cancer Dataset

Now that we’ve built up the tools to build a Logistic Regression model for a classification dataset, we’ll introduce a new dataset.

In the breast cancer dataset, each datapoint has measurements from an image of a breast mass and whether or not it’s cancerous. The goal will be to use these measurements to predict if the mass is cancerous.

This dataset is built right into scikit-learn so we won’t need to read in a csv.

Let’s start by loading the dataset and taking a peak at the data and how it’s formatted.
from sklearn.datasets import load_breast_cancer
cancer_data = load_breast_cancer()

The object returned (which we stored in the cancer_data variable) is an object similar to a Python dictionary. We can see the available keys with the keys method.
print(cancer_data.keys())

We’ll start by looking at DESCR, which gives a detailed description of the dataset.
print(cancer_data['DESCR'])

Run this code to see the results:
import pandas as pd
from sklearn.datasets import load_breast_cancer

cancer_data = load_breast_cancer()
print(cancer_data.keys())
print(cancer_data['DESCR'])

```{python}
import pandas as pd
from sklearn.datasets import load_breast_cancer

cancer_data = load_breast_cancer()
print(cancer_data.keys())
print(cancer_data['DESCR'])
```

Loading the Data into Pandas

Let’s pull the feature and target data out of the cancer_data object.

First, the feature data is stored with the 'data' key. When we look at it, we see that it’s a numpy array with 569 rows and 30 columns. That’s because we have 569 datapoints and 30 features.

The following is a numpy array of the data.
cancer_data['data']

We use the shape to see that it is an array with 569 rows and 30 columns.
cancer_data['data'].shape
Try It Yourself

In order to put this in a Pandas DataFrame and make it more human readable, we want the column names. These are stored with the 'feature_names' key.
cancer_data['feature_names']
Try It Yourself

Now we can create a Pandas DataFrame with all our feature data.
df = pd.DataFrame(cancer_data['data'], columns=cancer_data['feature_names'])
print(df.head())
Try It Yourself

Result:


We can see that we have 30 columns in the DataFrame, since we have 30 features. The output is truncated so that it’ll fit on the screen. We used the head method, so our result only has 5 datapoints.

We still need to put the target data in our DataFrame, which can be found with the 'target' key. We can see that the target is a 1-dimensional numpy array of 1’s and 0’s.
cancer_data['target']
Try It Yourself

If we look at the shape of the array, we see that it’s a 1-dimensional array with 569 values (which was how many datapoints we had).
cancer_data['target'].shape
Try It Yourself

In order to interpret these 1’s and 0’s, we need to know whether 1 or 0 is benign or malignant. This is given by the target_names
cancer_data['target_names']
Try It Yourself

This gives the array ['malignant' 'benign'] which tells us that 0 means malignant and 1 means benign. Let’s add this data to the Pandas DataFrame.
df['target'] = cancer_data['target']
df.head()
Try It Yourself

Run this code to see the results:
import pandas as pd
from sklearn.datasets import load_breast_cancer

cancer_data = load_breast_cancer()

df = pd.DataFrame(cancer_data['data'], columns=cancer_data['feature_names'])
df['target'] = cancer_data['target']
print(df.head())

```{python}
cancer_data['data']
cancer_data['data'].shape
cancer_data['feature_names']
df = pd.DataFrame(cancer_data['data'],columns=cancer_data['feature_names'])
print(df.head())
cancer_data['target']
cancer_data['target'].shape
cancer_data['target_names']
df['target'] = cancer_data['target']
df.head()
```

```{python}
import pandas as pd
from sklearn.datasets import load_breast_cancer

cancer_data = load_breast_cancer()

df = pd.DataFrame(cancer_data['data'], columns=cancer_data['feature_names'])
df['target'] = cancer_data['target']
print(df.head())
```

Build a Logistic Regression Model

Now that we’ve taken a look at our data and gotten it into a comfortable format, we can build our feature matrix X and target array y so that we can build a Logistic Regression model.
X = df[cancer_data.feature_names].values
y = df['target'].values

Now we create a Logistic Regression object and use the fit method to build the model.
model = LogisticRegression()
model.fit(X, y)

When we run this code we get a Convergence Warning. This means that the model needs more time to find the optimal solution. One option is to increase the number of iterations. You can also switch to a different solver, which is what we will do. The solver is the algorithm that the model uses to find the equation of the line. You can see the possible solvers in the Logistic Regression documentation.
model = LogisticRegression(solver='liblinear')
model.fit(X, y) 

Let’s see what the model predicts for the first datapoint in our dataset. Recall that the predict method takes a 2-dimensional array so we must put the datapoint in a list.
model.predict([X[0]])

So the model predicts that the first datapoint is benign.

To see how well the model performs over the whole dataset, we use the score method to see the accuracy of the model.
model.score(X, y)

Run this code to see the results:
import pandas as pd
from sklearn.datasets import load_breast_cancer
from sklearn.linear_model import LogisticRegression

cancer_data = load_breast_cancer()
df = pd.DataFrame(cancer_data['data'], columns=cancer_data['feature_names'])
df['target'] = cancer_data['target']

X = df[cancer_data.feature_names].values
y = df['target'].values

model = LogisticRegression(solver='liblinear')
model.fit(X, y)
print("prediction for datapoint 0:", model.predict([X[0]]))
print(model.score(X, y))

```{python}
import pandas as pd
from sklearn.datasets import load_breast_cancer
from sklearn.linear_model import LogisticRegression

cancer_data = load_breast_cancer()
df = pd.DataFrame(cancer_data['data'],columns=cancer_data['feature_names'])
df['target'] = cancer_data['target']

X = df[cancer_data.feature_names].values
y = df['target'].values

model = LogisticRegression(solver='liblinear')
model.fit(X, y)
print("prediction for datapoint 0:", model.predict([X[0]]))
print(model.score(X, y))

```

Accuracy, Precision, Recall & F1 Score in Sklearn

Scikit-learn has a function built in for each of the metrics that we have introduced. We have a separate function for each of the accuracy, precision, recall and F1 score.

In order to use them, let’s start by recalling our code from the previous module to build a Logistic Regression model. The code reads in the Titanic dataset from the csv file and puts it in a Pandas DataFrame. Then we create a feature matrix X and target values y. We create a Logistic Regression model and fit it to our dataset. Finally, we create a variable y_pred of our predictions.
df = pd.read_csv('https://sololearn.com/uploads/files/titanic.csv')
df['male'] = df['Sex'] == 'male'
X = df[['Pclass', 'male', 'Age', 'Siblings/Spouses', 'Parents/Children', 'Fare']].values
y = df['Survived'].values
model = LogisticRegression()
model.fit(X, y)
y_pred = model.predict(X)

Now we’re ready to use our metric functions. Let’s import them from scikit-learn.
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

Each function takes two 1-dimensional numpy arrays: the true values of the target & the predicted values of the target. We have the true values of the target and the predicted values of the target. Thus we can use the metric functions as follows.
print("accuracy:", accuracy_score(y, y_pred))
print("precision:", precision_score(y, y_pred))
print("recall:", recall_score(y, y_pred))
print("f1 score:", f1_score(y, y_pred))

```{python}
df = pd.read_csv('titanic.csv')
df['male'] = df['Sex'] == 'male'
X = df[['Pclass', 'male', 'Age', 'Siblings/Spouses','Parents/Children', 'Fare']].values
y = df['Survived'].values
model = LogisticRegression()
model.fit(X, y)
y_pred = model.predict(X)

from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

print("accuracy:", accuracy_score(y, y_pred))
print("precision:", precision_score(y, y_pred))
print("recall:", recall_score(y, y_pred))
print("f1 score:", f1_score(y, y_pred))

```

Confusion Matrix in Sklearn

Scikit-learn has a confusion matrix function that we can use to get the four values in the confusion matrix (true positives, false positives, false negatives, and true negatives). Assuming y is our true target values and y_pred is the predicted values, we can use the confusion_matrix function as follows:
from sklearn.metrics import confusion_matrix
print(confusion_matrix(y, y_pred))
# Output:
# [[475  70]
#  [103 239]]

```{python}
from sklearn.metrics import confusion_matrix
print(confusion_matrix(y, y_pred))
```

Training and Testing in Sklearn

Scikit-learn has a function built in for splitting the data into a training set and a test set.

Assuming we have a 2-dimensional numpy array X of our features and a 1-dimensional numpy array y of the target, we can use the train_test_split function. It will randomly put each datapoint in either the training set or the test set. By default the training set is 75% of the data and the test set is the remaining 25% of the data.
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y)

Let’s use the shape attribute to see the sizes of our datasets.
print("whole dataset:", X.shape, y.shape)
print("training set:", X_train.shape, y_train.shape)
print("test set:", X_test.shape, y_test.shape)


Run this code to see the results:
import pandas as pd
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split

df = pd.read_csv('https://sololearn.com/uploads/files/titanic.csv')
df['male'] = df['Sex'] == 'male'
X = df[['Pclass', 'male', 'Age', 'Siblings/Spouses', 'Parents/Children', 'Fare']].values
y = df['Survived'].values

X_train, X_test, y_train, y_test = train_test_split(X, y)

print("whole dataset:", X.shape, y.shape)
print("training set:", X_train.shape, y_train.shape)
print("test set:", X_test.shape, y_test.shape)

```{python}
import pandas as pd
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split

df = pd.read_csv('titanic.csv')
df['male'] = df['Sex'] == 'male'
X = df[['Pclass', 'male', 'Age', 'Siblings/Spouses', 'Parents/Children', 'Fare']].values
y = df['Survived'].values

X_train, X_test, y_train, y_test = train_test_split(X, y)

print("whole dataset:", X.shape, y.shape)
print("training set:", X_train.shape, y_train.shape)
print("test set:", X_test.shape, y_test.shape)
```

Building a Scikit-learn Model Using a Training Set

Now that we know how to split our data into a training set and a test set, we need to modify how we build and evaluate the model. All of the model building is done with the training set and all of the evaluation is done with the test set.

In the last module, we built a model and evaluated it on the same dataset. Now we build the model using the training set.
model = LogisticRegression()
model.fit(X_train, y_train)

And we evaluate the model using the test set.
print(model.score(X_test, y_test))

In fact, all of the metrics we calculate in the previous parts should be calculated on the test set.
y_pred = model.predict(X_test)
print("accuracy:", accuracy_score(y_test, y_pred))
print("precision:", precision_score(y_test, y_pred))
print("recall:", recall_score(y_test, y_pred))
print("f1 score:", f1_score(y_test, y_pred))

```{python}
model = LogisticRegression()
model.fit(X_train, y_train)

print(model.score(X_test, y_test))

y_pred = model.predict(X_test)
print("accuracy:", accuracy_score(y_test, y_pred))
print("precision:", precision_score(y_test, y_pred))
print("recall:", recall_score(y_test, y_pred))
print("f1 score:", f1_score(y_test, y_pred))
```

Using a Random State

As we noticed in the previous part, when we randomly split the data into a training set and a test set, we end up with different datapoints in each set each time we run the code. This is a result of randomness, and we need it to be random for it to be effective, but this can sometimes make it difficult to test the code.

For example, each time we run the following code, we will get different results.
from sklearn.model_selection import train_test_split

X = [[1, 1], [2, 2], [3, 3], [4, 4]]
y = [0, 0, 1, 1]

X_train, X_test, y_train, y_test = train_test_split(X, y)
print('X_train', X_train)
print('X_test', X_test)

```{python}
from sklearn.model_selection import train_test_split

X = [[1, 1], [2, 2], [3, 3], [4, 4]]
y = [0, 0, 1, 1]

X_train, X_test, y_train, y_test = train_test_split(X, y)
print('X_train', X_train)
print('X_test', X_test)
```

To get the same split every time, we can use the random_state attribute. We choose an arbitrary number to give it, and then every time we run the code, we will get the same split.
from sklearn.model_selection import train_test_split

X = [[1, 1], [2, 2], [3, 3], [4, 4]]
y = [0, 0, 1, 1]

X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=27)
print('X_train', X_train)
print('X_test', X_test)

```{python}
from sklearn.model_selection import train_test_split

X = [[1, 1], [2, 2], [3, 3], [4, 4]]
y = [0, 0, 1, 1]

X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=27)
print('X_train', X_train)
print('X_test', X_test)
```

Precision = TP / (TP + FP) 
  #we want a low amount false positives
Sensitivity = Recall = TP / (TP + FN) 
  #we want a low amount of false negatives
Specificity = TN / (TN + FP)
  #we want to catch the true negatives
Accuracy = (TP + TN) / total Cases 
  #we want a low amount of wrong predictions

Goal: Max Sensitivity and Specificity

Sensitivity & Specificity in Scikit-learn

Scikit-learn has not defined functions for sensitivity and specificity, but we can do it ourselves. Sensitivity is the same as recall, so it is easy to define.
from sklearn.metrics import recall_score
sensitivity_score = recall_score
print(sensitivity_score(y_test, y_pred)) 
# 0.6829268292682927

Now, to define specificity, if we realize that it is also the recall of the negative class, we can get the value from the sklearn function precision_recall_fscore_support.

Let’s look at the output of precision_recall_fscore_support.
from sklearn.metrics import precision_recall_fscore_support
print(precision_recall_fscore_support(y, y_pred))

The second array is the recall, so we can ignore the other three arrays. There are two values. The first is the recall of the negative class and the second is the recall of the positive class. The second value is the standard recall or sensitivity value, and you can see the value matches what we got above. The first value is the specificity. So let’s write a function to get just that value.
def specificity_score(y_true, y_pred):
    p, r, f, s = precision_recall_fscore_support(y_true, y_pred)
    return r[0]
print(specificity_score(y_test, y_pred)) 
# 0.9214285714285714

Note that in the code sample we use a random state in the train test split so that every time you run the code you will get the same results.
import pandas as pd
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import recall_score, precision_recall_fscore_support

sensitivity_score = recall_score
def specificity_score(y_true, y_pred):
    p, r, f, s = precision_recall_fscore_support(y_true, y_pred)
    return r[0]

df = pd.read_csv('https://sololearn.com/uploads/files/titanic.csv')
df['male'] = df['Sex'] == 'male'
X = df[['Pclass', 'male', 'Age', 'Siblings/Spouses', 'Parents/Children', 'Fare']].values
y = df['Survived'].values

X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=5)

model = LogisticRegression()
model.fit(X_train, y_train)
y_pred = model.predict(X_test)

print("sensitivity:", sensitivity_score(y_test, y_pred))
print("specificity:", specificity_score(y_test, y_pred))

```{python}
import pandas as pd
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import recall_score, precision_recall_fscore_support

sensitivity_score = recall_score
def specificity_score(y_true, y_pred):
    p, r, f, s = precision_recall_fscore_support(y_true, y_pred)
    return r[0]

df = pd.read_csv('titanic.csv')
df['male'] = df['Sex'] == 'male'
X = df[['Pclass', 'male', 'Age', 'Siblings/Spouses', 'Parents/Children', 'Fare']].values
y = df['Survived'].values

X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=5)

model = LogisticRegression()
model.fit(X_train, y_train)
y_pred = model.predict(X_test)

print("sensitivity:", sensitivity_score(y_test, y_pred))
print("specificity:", specificity_score(y_test, y_pred))
```

Adjusting the Logistic Regression Threshold in Sklearn

When you use scikit-learn’s predict method, you are given 0 and 1 values of the prediction. However, behind the scenes the Logistic Regression model is getting a probability value between 0 and 1 for each datapoint and then rounding to either 0 or 1. If we want to choose a different threshold besides 0.5, we’ll want those probability values. We can use the predict_proba function to get them.
(model.predict_proba(X_test)

The result is a numpy array with 2 values for each datapoint (e.g. [0.78, 0.22]). You’ll notice that the two values sum to 1. The first value is the probability that the datapoint is in the 0 class (didn’t survive) and the second is the probability that the datapoint is in the 1 class (survived). We only need the second column of this result, which we can pull with the following numpy syntax.
model.predict_proba(X_test)[:, 1]

Now we just want to compare these probability values with our threshold. Say we want a threshold of 0.75. We compare the above array to 0.75. This will give us an array of True/False values which will be our array of predicted target values.
y_pred = model.predict_proba(X_test)[:, 1] > 0.75

A threshold of 0.75 means we need to be more confident in order to make a positive prediction. This results in fewer positive predictions and more negative predictions.

Now we can use any scikit-learn metrics from before using y_test as our true values and y_pred as our predicted values.
print("precision:", precision_score(y_test, y_pred))
print("recall:", recall_score(y_test, y_pred))

Run this code to see the results:
import pandas as pd
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import precision_score, recall_score
from sklearn.model_selection import train_test_split

df = pd.read_csv('https://sololearn.com/uploads/files/titanic.csv')
df['male'] = df['Sex'] == 'male'
X = df[['Pclass', 'male', 'Age', 'Siblings/Spouses', 'Parents/Children', 'Fare']].values
y = df['Survived'].values

X_train, X_test, y_train, y_test = train_test_split(X, y)

model = LogisticRegression()
model.fit(X_train, y_train)

print("predict proba:")
print(model.predict_proba(X_test))

y_pred = model.predict_proba(X_test)[:, 1] > 0.75

print("precision:", precision_score(y_test, y_pred))
print("recall:", recall_score(y_test, y_pred))

```{python}
import pandas as pd
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import precision_score, recall_score
from sklearn.model_selection import train_test_split

df = pd.read_csv('titanic.csv')
df['male'] = df['Sex'] == 'male'
X = df[['Pclass', 'male', 'Age', 'Siblings/Spouses', 'Parents/Children', 'Fare']].values
y = df['Survived'].values

X_train, X_test, y_train, y_test = train_test_split(X, y)

model = LogisticRegression()
model.fit(X_train, y_train)

print("predict proba:")
print(model.predict_proba(X_test))

y_pred = model.predict_proba(X_test)[:, 1] > 0.75

print("precision:", precision_score(y_test, y_pred))
print("recall:", recall_score(y_test, y_pred))
```

How to Build an ROC Curve

The ROC curve is a graph of the specificity vs the sensitivity. We build a Logistic Regression model and then calculate the specificity and sensitivity for every possible threshold. Every predicted probability is a threshold. If we have 5 datapoints with the following predicted probabilities: 0.3, 0.4, 0.6, 0.7, 0.8, we would use each of those 5 values as a threshold.

Note that we actually plot the sensitivity vs (1-specificity). There is no strong reason for doing it this way besides that it’s the standard.

Let’s start by looking at the code to build the ROC curve. Scikit-learn has a roc_curve function we can use. The function takes the true target values and the predicted probabilities from our model.

We first use the predict_proba method on the model to get the probabilities. Then we call the roc_curve function. The roc_curve function returns an array of the false positive rates, an array of the true positive rates and the thresholds. The false positive rate is 1-specificity (x-axis) and the true positive rate is another term for the sensitivity (y-axis). The threshold values won’t be needed in the graph.

Here’s the code for plotting the ROC curve in matplotlib. Note that we also have code for plotting a diagonal line. This can help us visually see how far our model is from a model that predicts randomly.

We assume that we already have a dataset that has been split into a training set and test set.
model = LogisticRegression()
model.fit(X_train, y_train)
y_pred_proba = model.predict_proba(X_test)
fpr, tpr, thresholds = roc_curve(y_test, y_pred_proba[:,1])

plt.plot(fpr, tpr)
plt.plot([0, 1], [0, 1], linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.0])
plt.xlabel('1 - specificity')
plt.ylabel('sensitivity')
plt.show()

```{python}
from sklearn import metrics
model = LogisticRegression()
model.fit(X_train, y_train)
y_pred_proba = model.predict_proba(X_test)
fpr, tpr, thresholds = metrics.roc_curve(y_test, y_pred_proba[:,1])

plt.plot(fpr, tpr)
plt.plot([0, 1], [0, 1], linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.0])
plt.xlabel('1 - specificity')
plt.ylabel('sensitivity')
plt.show()
```

Maximize true negatives (Specificity)
Maximize true positives (Sensitivity)

Area Under the Curve

We’ll sometimes what to use the ROC curve to compare two different models. Here is a comparison of the ROC curves of two models.


You can see that the blue curve outperforms the orange one since the blue line is almost always above the orange line.

To get an empirical measure of this, we calculate the Area Under the Curve, also called the AUC. This is the area under the ROC curve. It’s a value between 0 and 1, the higher the better.

Since the ROC is a graph of all the different Logistic Regression models with different thresholds, the AUC does not measure the performance of a single model. It gives a general sense of how well the Logistic Regression model is performing. To get a single model, you still need to find the optimal threshold for your problem.

Let’s use scikit-learn to help us calculate the area under the curve. We can use the roc_auc_score function.
(roc_auc_score(y_test, y_pred_proba[:,1]) 

Here are the values for the two lines:
Blue AUC: 0.8379
Orange AUC: 0.7385

You can see empirically that the blue is better.

We can use the roc_auc_score function to calculate the AUC score of a Logistic Regression model on the Titanic dataset. We build two Logistic Regression models, model1 with 6 features and model2 with just Pclass and male features. We see that the AUC score of model1 is higher.

Run this code to see the results:
import pandas as pd
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import roc_auc_score

df = pd.read_csv('https://sololearn.com/uploads/files/titanic.csv')
df['male'] = df['Sex'] == 'male'
X = df[['Pclass', 'male', 'Age', 'Siblings/Spouses', 'Parents/Children', 'Fare']].values
y = df['Survived'].values

X_train, X_test, y_train, y_test = train_test_split(X, y)

model1 = LogisticRegression()
model1.fit(X_train, y_train)
y_pred_proba1 = model1.predict_proba(X_test)
print("model 1 AUC score:", roc_auc_score(y_test, y_pred_proba1[:, 1]))

model2 = LogisticRegression()
model2.fit(X_train[:, 0:2], y_train)
y_pred_proba2 = model2.predict_proba(X_test[:, 0:2])
print("model 1 AUC score:", roc_auc_score(y_test, y_pred_proba2[:, 1]))

```{python}
import pandas as pd
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import roc_auc_score

df = pd.read_csv('titanic.csv')
df['male'] = df['Sex'] == 'male'
X = df[['Pclass', 'male', 'Age', 'Siblings/Spouses', 'Parents/Children', 'Fare']].values
y = df['Survived'].values

X_train, X_test, y_train, y_test = train_test_split(X, y)

model1 = LogisticRegression()
model1.fit(X_train, y_train)
y_pred_proba1 = model1.predict_proba(X_test)
print("model 1 AUC score:", roc_auc_score(y_test, y_pred_proba1[:, 1]))

model2 = LogisticRegression()
model2.fit(X_train[:, 0:2], y_train)
y_pred_proba2 = model2.predict_proba(X_test[:, 0:2])
print("model 1 AUC score:", roc_auc_score(y_test, y_pred_proba2[:, 1]))
```

Concerns with Training and Test Set

We are doing evaluation because we want to get an accurate measure of how well the model performs. If our dataset is small, our test set is going to be small. Thus it might not be a good random assortment of datapoints and by random chance end up with easy or difficult datapoints in our evaluation set.

Since our goal is to get the best possible measure of our metrics (accuracy, precision, recall and F1 score), we can do a little better than just a single training and test set.

Recall that our training and test set split looks as follows.


As we can see, all the values in the training set are never used to evaluate. It would be unfair to build the model with the training set and then evaluate with the training set, but we are not getting as full a picture of the model performance as possible.

To see this empirically, let’s try running the code from Lesson 3 which does a train/test split. We’ll re-run it a few times and see the results. Each row is the result of a different random train/test split.


You can see that each time we run it, we get different values for the metrics. The accuracy ranges from 0.79 to 0.84, the precision from 0.75 to 0.81 and the recall from 0.63 to 0.75. These are wide ranges that just depend on how lucky or unlucky we were in which datapoints ended up in the test set.

Here’s the code if you want to try running yourself and seeing the varying values of the metrics.
import pandas as pd
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
from sklearn.model_selection import train_test_split
import numpy as np

df = pd.read_csv('https://sololearn.com/uploads/files/titanic.csv')
df['male'] = df['Sex'] == 'male'
X = df[['Pclass', 'male', 'Age', 'Siblings/Spouses', 'Parents/Children', 'Fare']].values
y = df['Survived'].values

X_train, X_test, y_train, y_test = train_test_split(X, y)

# building the model
model = LogisticRegression()
model.fit(X_train, y_train)

# evaluating the model
y_pred = model.predict(X_test)
print(" accuracy: {0:.5f}".format(accuracy_score(y_test, y_pred)))
print("precision: {0:.5f}".format(precision_score(y_test, y_pred)))
print("   recall: {0:.5f}".format(recall_score(y_test, y_pred)))
print(" f1 score: {0:.5f}".format(f1_score(y_test, y_pred)))

```{python}
import pandas as pd
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
from sklearn.model_selection import train_test_split
import numpy as np

df = pd.read_csv('https://sololearn.com/uploads/files/titanic.csv')
df['male'] = df['Sex'] == 'male'
X = df[['Pclass', 'male', 'Age', 'Siblings/Spouses', 'Parents/Children', 'Fare']].values
y = df['Survived'].values

X_train, X_test, y_train, y_test = train_test_split(X, y)

# building the model
model = LogisticRegression()
model.fit(X_train, y_train)

# evaluating the model
y_pred = model.predict(X_test)
print(" accuracy: {0:.5f}".format(accuracy_score(y_test, y_pred)))
print("precision: {0:.5f}".format(precision_score(y_test, y_pred)))
print("   recall: {0:.5f}".format(recall_score(y_test, y_pred)))
print(" f1 score: {0:.5f}".format(f1_score(y_test, y_pred)))
```

KFold Class

Scikit-learn has already implemented the code to break the dataset into k chunks and create k training and test sets.

For simplicity, let’s take a dataset with just 6 datapoints and 2 features and a 3-fold cross validation on the dataset. We’ll take the first 6 rows from the Titanic dataset and use just the Age and Fare columns.
X = df[['Age', 'Fare']].values[:6]
y = df['Survived'].values[:6]

We start by instantiating a KFold class object. It takes two parameters: n_splits (this is k, the number of chunks to create) and shuffle (whether or not to randomize the order of the data). It’s generally good practice to shuffle the data since you often get a dataset that’s in a sorted order.
kf = KFold(n_splits=3, shuffle=True)

The KFold class has a split method that creates the 3 splits for our data.

Let’s look at the output of the split method. The split method returns a generator, so we use the list function to turn it into a list.
list(kf.split(X))

Result:
from sklearn.model_selection import KFold
import pandas as pd

df = pd.read_csv('https://sololearn.com/uploads/files/titanic.csv')
X = df[['Age', 'Fare']].values[:6]
y = df['Survived'].values[:6]

kf = KFold(n_splits=3, shuffle=True)
for train, test in kf.split(X):
    print(train, test)
    
```{python}
from sklearn.model_selection import KFold
import pandas as pd

df = pd.read_csv('titanic.csv')
X = df[['Age', 'Fare']].values[:6]
y = df['Survived'].values[:6]

kf = KFold(n_splits=3, shuffle=True)
for train, test in kf.split(X):
    print(train, test)

```

Creating Training and Test Sets with the Folds

We used the KFold class and split method to get the indices that are in each of the splits. Now let’s use that result to get our first (of 3) train/test splits.

First let’s pull out the first split.
splits = list(kf.split(X))
first_split = splits[0]
print(first_split)
# (array([0, 2, 3, 5]), array([1, 4]))

The first array is the indices for the training set and the second is the indices for the test set. Let’s create these variables.
train_indices, test_indices = first_split
print("training set indices:", train_indices)
print("test set indices:", test_indices)
# training set indices: [0, 2, 3, 5]
# test set indices: [1, 4]

Now we can create an X_train, y_train, X_test, and y_test based on these indices.
X_train = X[train_indices]
X_test = X[test_indices]
y_train = y[train_indices]
y_test = y[test_indices]

If we print each of these out, we’ll see that we have four of the datapoints in X_train and their target values in y_train. The remaining 2 datapoints are in X_test and their target values in y_test.
print("X_train")
print(X_train)
print("y_train", y_train)
print("X_test")
print(X_test)
print("y_test", y_test)

Run this code to see the results:
from sklearn.model_selection import KFold
import pandas as pd
df = pd.read_csv('https://sololearn.com/uploads/files/titanic.csv')
X = df[['Age', 'Fare']].values[:6]
y = df['Survived'].values[:6]

kf = KFold(n_splits=3, shuffle=True)

splits = list(kf.split(X))
first_split = splits[0]
train_indices, test_indices = first_split
print("training set indices:", train_indices)
print("test set indices:", test_indices)

X_train = X[train_indices]
X_test = X[test_indices]
y_train = y[train_indices]
y_test = y[test_indices]
print("X_train")
print(X_train)
print("y_train", y_train)
print("X_test")
print(X_test)
print("y_test", y_test)

```{python}
from sklearn.model_selection import KFold
import pandas as pd
df = pd.read_csv('https://sololearn.com/uploads/files/titanic.csv')
X = df[['Age', 'Fare']].values[:6]
y = df['Survived'].values[:6]

kf = KFold(n_splits=3, shuffle=True)

splits = list(kf.split(X))
first_split = splits[0]
train_indices, test_indices = first_split
print("training set indices:", train_indices)
print("test set indices:", test_indices)

X_train = X[train_indices]
X_test = X[test_indices]
y_train = y[train_indices]
y_test = y[test_indices]
print("X_train")
print(X_train)
print("y_train", y_train)
print("X_test")
print(X_test)
print("y_test", y_test)
```

Build a Model

Now we can use the training and test sets to build a model and make a prediction like before. Let’s go back to using the entire dataset (since 4 datapoints is not enough to build a decent model).

Here’s the entirety of the code to build and score the model on the first fold of a 5-fold cross validation. Note that the code for fitting and scoring the model is exactly the same as it was when we used the train_test_split function.

Try it now:
from sklearn.model_selection import KFold
from sklearn.linear_model import LogisticRegression
import pandas as pd

df = pd.read_csv('https://sololearn.com/uploads/files/titanic.csv')
df['male'] = df['Sex'] == 'male'
X = df[['Pclass', 'male', 'Age', 'Siblings/Spouses', 'Parents/Children', 'Fare']].values
y = df['Survived'].values

kf = KFold(n_splits=5, shuffle=True)

splits = list(kf.split(X))
train_indices, test_indices = splits[0]
X_train = X[train_indices]
X_test = X[test_indices]
y_train = y[train_indices]
y_test = y[test_indices]

model = LogisticRegression()
model.fit(X_train, y_train)
print(model.score(X_test, y_test))

```{python}
from sklearn.model_selection import KFold
from sklearn.linear_model import LogisticRegression
import pandas as pd

df = pd.read_csv('titanic.csv')
df['male'] = df['Sex'] == 'male'
X = df[['Pclass', 'male', 'Age', 'Siblings/Spouses', 'Parents/Children', 'Fare']].values
y = df['Survived'].values

kf = KFold(n_splits=5, shuffle=True)

splits = list(kf.split(X))
train_indices, test_indices = splits[0]
X_train = X[train_indices]
X_test = X[test_indices]
y_train = y[train_indices]
y_test = y[test_indices]

model = LogisticRegression()
model.fit(X_train, y_train)
print(model.score(X_test, y_test))
```

Loop Over All the Folds

We have been doing one fold at a time, but really we want to loop over all the folds to get all the values. We will put the code from the previous part inside our for loop.
scores = []
kf = KFold(n_splits=5, shuffle=True)
for train_index, test_index in kf.split(X):
    X_train, X_test = X[train_index], X[test_index]
    y_train, y_test = y[train_index], y[test_index]
    model = LogisticRegression()
    model.fit(X_train, y_train)
    scores.append(model.score(X_test, y_test))
print(scores)
# [0.75847, 0.83146, 0.85876, 0.76271, 0.74011]

Since we have 5 folds, we get 5 accuracy values. Recall, to get a single final value, we need to take the mean of those values.
print(np.mean(scores))
# 0.79029

Now that we’ve calculated the accuracy, we no longer need the 5 different models that we’ve built. For future use, we just want a single model. To get the single best possible model, we build a model on the whole dataset. If we’re asked the accuracy of this model, we use the accuracy calculated by cross validation (0.79029) even though we haven’t actually tested this particular model with a test set.
final_model = LogisticRegression()
final_model.fit(X, y)

```{python}
scores = []
kf = KFold(n_splits=5, shuffle=True)
for train_index, test_index in kf.split(X):
    X_train, X_test = X[train_index], X[test_index]
    y_train, y_test = y[train_index], y[test_index]
    model = LogisticRegression()
    model.fit(X_train, y_train)
    scores.append(model.score(X_test, y_test))
print(scores)
# [0.75847, 0.83146, 0.85876, 0.76271, 0.74011]

print(np.mean(scores))
# 0.79029
```


First, we import the necessary modules and prep the data as we’ve done before.

```{python}
from sklearn.model_selection import KFold
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
import pandas as pd
import numpy as np

df = pd.read_csv('https://sololearn.com/uploads/files/titanic.csv')
df['male'] = df['Sex'] == 'male'
```

Now we can build the KFold object. We’ll use 5 splits as that’s standard. Note that we want to create a single KFold object that all of the models will use. It would be unfair if different models got a different split of the data.

```{python}
kf = KFold(n_splits=5, shuffle=True)
```

Now we’ll create three different feature matrices X1, X2 and X3. All will have the same target y.

```{python}
X1 = df[['Pclass', 'male', 'Age', 'Siblings/Spouses', 'Parents/Children', 'Fare']].values
X2 = df[['Pclass', 'male', 'Age']].values
X3 = df[['Fare', 'Age']].values
y = df['Survived'].values
```

Since we’ll be doing it several times, let’s write a function to score the model. This function uses the KFold object to calculate the accuracy, precision, recall and F1 score for a Logistic Regression model with the given feature matrix X and target array y.

```{python}
def score_model(X, y, kf):
    accuracy_scores = []
    precision_scores = []
    recall_scores = []
    f1_scores = []
    for train_index, test_index in kf.split(X):
        X_train, X_test = X[train_index], X[test_index]
        y_train, y_test = y[train_index], y[test_index]
        model = LogisticRegression()
        model.fit(X_train, y_train)
        y_pred = model.predict(X_test)
        accuracy_scores.append(accuracy_score(y_test, y_pred))
        precision_scores.append(precision_score(y_test, y_pred))
        recall_scores.append(recall_score(y_test, y_pred))
        f1_scores.append(f1_score(y_test, y_pred))
    print("accuracy:", np.mean(accuracy_scores))
    print("precision:", np.mean(precision_scores))
    print("recall:", np.mean(recall_scores))
    print("f1 score:", np.mean(f1_scores))
    
```

Then we call our function three times for each of our three feature matrices and see the results.

```{python}
print("Logistic Regression with all features")
score_model(X1, y, kf)
print()
print("Logistic Regression with Pclass, Sex & Age features")
score_model(X2, y, kf)
print()
print("Logistic Regression with Fare & Age features")
score_model(X3, y, kf)

```

Since the first two models have equivalent results, it makes sense to choose the simpler model, the one that uses the Pclass, Sex & Age features.

Now that we’ve made a choice of a best model, we build a single final model using all of the data.

```{python}
model = LogisticRegression()
model.fit(X1, y)
```

Now we can make a prediction with our model.

```{python}
model.predict([[3, False, 25, 0, 1, 2]]) 
# Output: [1]
```

#############A Nonparametric Machine Learning Algorithm

So far we’ve been dealing with Logistic Regression. In Logistic Regression, we look at the data graphically and draw a line to separate the data. The model is defined by the coefficients that define the line. These coefficients are called parameters. Since the model is defined by these parameters, Logistic Regression is a parametric machine learning algorithm.

In this module, we’ll introduce Decision Trees, which are an example of a nonparametric machine learning algorithm. Decision Trees won’t be defined by a list of parameters as we’ll see in the upcoming lessons.

###Tree Terminology

The reason many people love decision trees is because they are very easy to interpret. It is basically a flow chart of questions that you answer about a datapoint until you get to a prediction.

Here’s an example of a Decision Tree for the Titanic dataset. We’ll see in the next lesson how this tree is constructed.

Each of the rectangles is called a node. The nodes which have a feature to split on are called internal nodes. The very first internal node at the top is called the root node. The final nodes where we make the predictions of survived/didn’t survive are called leaf nodes. Internal nodes all have two nodes below them, which we call the node’s children.


#### Look for Homgeneity

A factor where there is the biggest discrepancy 

#### Look for Gini impurity

p is the percent of hits on one half of the split 
vs 
q the percent of hist on the other half of the split

gini = 2*p*q: .0 pure to .5 impure

#### Look for entropy

entropy = -[plog2p + qlog2q]

#### Look for Information Gain

IG = H(S)-|A|/|S|*H(A)-|B|/|S|*H(B)

## DecisionTreeClassifier Class

Just like with Logistic Regression, scikit-learn has a Decision Tree class. The code for building a Decision Tree model is very similar to building a Logistic Regression model. Scikit-learn did this intentionally so that it is easy to build and compare different models for the same dataset.

Here’s the import statement.

```{python}
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split

df = pd.read_csv('titanic.csv')
df['male'] = df['Sex'] == 'male'
X = df[['Pclass', 'male', 'Age', 'Siblings/Spouses', 'Parents/Children', 'Fare']].values
y = df['Survived'].values
```

Now we can apply the same methods that we used with the LogisticRegression class: fit (to train the model), score (to calculate the accuracy score) and predict (to make predictions).

We first create a DecisionTreeClassifier object.

```{python}
model = DecisionTreeClassifier()
```

We do a train/test split using a random_state so that every time we run the code we will get the same split.

```{python}
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=22)
```

Then we use the fit method to train the model.

```{python}
model.fit(X_train, y_train)
```

We can use the predict method to see what the model predicts. Here we can see the prediction for a male passenger in Pclass 3, who is 22 years old, has 1 sibling/spouse on board, has 0 parents/children on board, and paid a fare of 7.25.

```{python}
print(model.predict([[3, True, 22, 1, 0, 7.25]]))
# [0]
```

We see that the model predicts that the passenger did not survive. This is the same prediction that the Logistic Regression model gave.

We can use the score and predict methods to get the accuracy, precision and recall scores.

```{python}
import pandas as pd
from sklearn.tree import DecisionTreeClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import KFold
from sklearn.metrics import precision_score, recall_score
import numpy as np

df = pd.read_csv('titanic.csv')
df['male'] = df['Sex'] == 'male'
X = df[['Pclass', 'male', 'Age', 'Siblings/Spouses', 'Parents/Children', 'Fare']].values
y = df['Survived'].values

kf = KFold(n_splits=5, shuffle=True, random_state=10)
dt_accuracy_scores = []
dt_precision_scores = []
dt_recall_scores = []
lr_accuracy_scores = []
lr_precision_scores = []
lr_recall_scores = []
for train_index, test_index in kf.split(X):
    X_train, X_test = X[train_index], X[test_index]
    y_train, y_test = y[train_index], y[test_index]
    dt = DecisionTreeClassifier()
    dt.fit(X_train, y_train)
    dt_accuracy_scores.append(dt.score(X_test, y_test))
    dt_y_pred = dt.predict(X_test)
    dt_precision_scores.append(precision_score(y_test, dt_y_pred))
    dt_recall_scores.append(recall_score(y_test, dt_y_pred))
    lr = LogisticRegression()
    lr.fit(X_train, y_train)
    lr_accuracy_scores.append(lr.score(X_test, y_test))
    lr_y_pred = lr.predict(X_test)
    lr_precision_scores.append(precision_score(y_test, lr_y_pred))
    lr_recall_scores.append(recall_score(y_test, lr_y_pred))
print("Decision Tree")
print("  accuracy:", np.mean(dt_accuracy_scores))
print("  precision:", np.mean(dt_precision_scores))
print("  recall:", np.mean(dt_recall_scores))
print("Logistic Regression")
print("  accuracy:", np.mean(lr_accuracy_scores))
print("  precision:", np.mean(lr_precision_scores))
print("  recall:", np.mean(lr_recall_scores))
```

Gini vs Entropy

The default impurity criterion in scikit-learn’s Decision Tree algorithm is the Gini Impurity. However, they’ve also implemented entropy and you can choose which one you’d like to use when you create the DecisionTreeClassifier object.

```{python}
dt = DecisionTreeClassifer(criterion='entropy')
```

Now we can compare a Decision Tree using gini with a Decision Tree using entropy. We first create a k-fold split since when we’re comparing two models we want them to use the same train/test splits to be fair. Then we do a k-fold cross validation with each of the two possible models. We calculate accuracy, precision and recall for each of the two options.

```{python}
import pandas as pd
import numpy as np
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import KFold
from sklearn.metrics import accuracy_score, precision_score, recall_score

df = pd.read_csv('titanic.csv')
df['male'] = df['Sex'] == 'male'
X = df[['Pclass', 'male', 'Age', 'Siblings/Spouses', 'Parents/Children', 'Fare']].values
y = df['Survived'].values

kf = KFold(n_splits=5, shuffle=True)
for criterion in ['gini', 'entropy']:
    print("Decision Tree - {}".format(criterion))
    accuracy = []
    precision = []
    recall = []
    for train_index, test_index in kf.split(X):
        X_train, X_test = X[train_index], X[test_index]
        y_train, y_test = y[train_index], y[test_index]
        dt = DecisionTreeClassifier(criterion=criterion)
        dt.fit(X_train, y_train)
        y_pred = dt.predict(X_test)
        accuracy.append(accuracy_score(y_test, y_pred))
        precision.append(precision_score(y_test, y_pred))
        recall.append(recall_score(y_test, y_pred))
    print("accuracy:", np.mean(accuracy))
    print("precision:", np.mean(precision))
    print("recall:", np.mean(recall), '\n')
    print()
    
```

Visualizing Decision Trees

If you want to create a png image of your graph, like the ones shown in this module, you can use scikit-learn's export_graphviz function.

First we import it.

```{python}
from sklearn.tree import export_graphviz
import graphviz
from IPython.display import Image

feature_names = ['Pclass', 'male']
X = df[feature_names].values
y = df['Survived'].values

dt = DecisionTreeClassifier()
dt.fit(X, y)

dot_file = export_graphviz(dt, feature_names=feature_names)
graph = graphviz.Source(dot_file)
graph.render(filename='tree', format='png', cleanup=True)
```

dt = DecisionTreeClassifier(max_depth=3, min_samples_leaf=2, max_leaf_nodes=10)

Grid Search

We’re not going to be able to intuit best values for the pre-pruning parameters. In order to decide on which to use, we use cross validation and compare metrics. We could loop through our different options like we did in the Lesson on Decision Trees in Scikit-learn, but scikit-learn has a grid search class built in that will do this for us.

The class is called GridSearchCV. We start by importing it.

```{python}
import pandas as pd
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import GridSearchCV

df = pd.read_csv('titanic.csv')
df['male'] = df['Sex'] == 'male'
X = df[['Pclass', 'male', 'Age', 'Siblings/Spouses', 'Parents/Children', 'Fare']].values
y = df['Survived'].values

```

GridSearchCV has four parameters that we’ll use:
1. The model (in this case a DecisionTreeClassifier)
2. Param grid: a dictionary of the parameters names and all the possible values
3. What metric to use (default is accuracy)
4. How many folds for k-fold cross validation

Let’s create the param grid variable. We’ll give a list of all the possible values for each parameter that we want to try.

```{python}
param_grid = {
    'max_depth': [5, 15, 25],
    'min_samples_leaf': [1, 3],
    'max_leaf_nodes': [10, 20, 35, 50]}
```

Now we create the grid search object. We’ll use the above parameter grid, set the scoring metric to the F1 score, and do a 5-fold cross validation.

```{python}
dt = DecisionTreeClassifier()
gs = GridSearchCV(dt, param_grid, scoring='f1', cv=5)
```

Now we can fit the grid search object. This can take a little time to run as it’s trying every possible combination of the parameters.

```{python}
gs.fit(X, y)
```

Since we have 3 possible values for max_depth, 2 for min_samples_leaf and 4 for max_leaf_nodes, we have 3 * 2 * 4 = 24 different combinations to try:

max_depth: 5, min_samples_leaf: 1, max_leaf_nodes: 10
max_depth: 15, min_samples_leaf: 1, max_leaf_nodes: 10
max_depth: 25, min_samples_leaf: 1, max_leaf_nodes: 10
max_depth: 5, min_samples_leaf: 3, max_leaf_nodes: 10
...

We use the best_params_ attribute to see which model won.

```{python}
print("best params:", gs.best_params_)
# best parameters: {'max_depth': 15, 'max_leaf_nodes': 35, 'min_samples_leaf': 1}
```

Thus we see that the best model has a maximum depth of 15, maximum number of leaf nodes as 35 and minimum samples per leaf of 1.

The best_score_ attribute tells us the score of the winning model.

```{python}
print("best score:", gs.best_score_)
# best score: 0.775712742630669
```

#Random Forest

Review of Breast Cancer Dataset

We’ll be using the breast cancer dataset in this lesson. Recall that the dataset has measures of different attributes of a lump in breast tissue and a label of whether or not the tumor is cancerous.

Here’s the code for pulling the data from sklearn:

```{python}
import pandas as pd
from sklearn.datasets import load_breast_cancer

cancer_data = load_breast_cancer()
df = pd.DataFrame(cancer_data['data'], columns=cancer_data['feature_names'])
df['target'] = cancer_data['target']

X = df[cancer_data.feature_names].values
y = df['target'].values
print('data dimensions', X.shape)
```

Random Forest with Sklearn

The syntax for building and using a Random Forest model is the same as it was for Logistic Regression and Decision Trees. The builders of scikit-learn intentionally made it so that it would be easy to switch between and compare different models.

Here’s the import statement for importing the Random Forest model for classification.

```{python}
from sklearn.ensemble import RandomForestClassifier
```

We’ll first split the dataset into a training set and test set.

```{python}
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=101)
```

We have added the random state parameter here so that it will do the same split every time we run the code. Without the random state, we’d expect different datapoints in the training and testing sets each time we run the code which can make it harder to test the code.

Then we create the RandomForestClassifier object and use the fit method to build the model on the training set.

```{python}
rf = RandomForestClassifier()
rf.fit(X_train, y_train)
```

Now we can use the model to make a prediction. For example, let’s take the first row of the test set and see what the prediction is. Recall that the predict method takes an array of points, so even when we have just one point, we have to put it in a list.

```{python}
first_row = X_test[0]
print("prediction:", rf.predict([first_row]))
print("true value:", y_test[0])
# prediction: [1]
# true value: 1
```

These results mean that the model predicted that the lump was cancerous and that was correct.

We can use the score method to calculate the accuracy over the whole test set.

```{python}
print("random forest accuracy:", rf.score(X_test, y_test))
# random forest accuracy: 0.965034965034965
```

Thus the accuracy is 96.5%. We can see how this compares to the Decision Tree model.

```{python}
from sklearn.tree import DecisionTreeClassifier
dt = DecisionTreeClassifier()
dt.fit(X_train, y_train)
print("decision tree accuracy:", dt.score(X_test, y_test))
# decision tree accuracy: 0.9020979020979021
```

So the accuracy for the decision tree model is 90.2%, much worse than that for the random forest.

Random Forest Parameters

When you look at the scikit-learn docs for the RandomForestClassifier, you will see quite a few parameters that you can control. We will be looking at a few of them, but you can learn about all of them in the docs.

Since a random forest is made up of decision trees, we have all the same tuning parameters for prepruning as we did for decision trees: max_depth, min_samples_leaf, and max_leaf_nodes. With random forests, it is generally not important to tune these as overfitting is generally not an issue.

We will look at two new tuning parameters: n_estimators (the number of trees) and max_features (the number of features to consider at each split).

The default for the max features is the square root of p, where p is the number of features (or predictors). The default is generally a good choice for max features and we usually will not need to change it, but you can set it to a fixed number with the following code.

```{python}
rf = RandomForestClassifier(max_features=5)
```

The default number of estimators (decision trees) is 10. This often works well but may in some cases be too small. You can set it to another number as follows. We will see in the next parts how to choose the best value.

```{python}
rf = RandomForestClassifier(n_estimators=15)
```

Grid Search

If you recall from the Decision Tree module, scikit-learn has built in a Grid Search class to help us find the optimal choice of parameters.

Let’s use Grid Search to compare the performance of a random forest with different numbers of trees.

Recall that we need to define the parameter grid of the parameters we want to vary and give a list of the values to try.

```{python}
 param_grid = {'n_estimators': [10, 25, 50, 75, 100],}
```

Now we can create a Random Forest Classifier and a Grid Search. Recall that the Grid Search will do k-fold cross validation for us. We set cv=5 for 5-fold cross validation.

```{python}
from sklearn.model_selection import GridSearchCV

cancer_data = load_breast_cancer()
df = pd.DataFrame(cancer_data['data'], columns=cancer_data['feature_names'])
df['target'] = cancer_data['target']

X = df[cancer_data.feature_names].values
y = df['target'].values

rf = RandomForestClassifier()
gs = GridSearchCV(rf, param_grid, cv=5)
```

Now we use the fit method to run the grid search. The best parameters will be stored in the best_params_ attribute.

```{python}
gs.fit(X, y)
print("best params:", gs.best_params_)
# best params: {'n_estimators': 50}
```

These are the parameters which yield the highest accuracy as that is the default metric. Note that you may get slightly different results each time you run this as the random split in the 5 folds may affect which has the best accuracy score.

Accuracy will work okay for us in this case as the classes in the breast cancer dataset are reasonably balanced. If the classes are imbalanced, we would want to use an alternative metric, like the f1-score. We can change the metric by scoring parameter to "f1" as follows. To avoid outputting a different best parameter each time, one can set the random_state in the classifier.

```{python}
rf = RandomForestClassifier(random_state=123)
gs = GridSearchCV(rf, param_grid, scoring='f1', cv=5)
gs.fit(X, y)
print("best params:", gs.best_params_)
# best params: {'n_estimators': 25}
```

Elbow Graph

With a parameter like the number of trees in a random forest, increasing the number of trees will never hurt performance. Increasing the number trees will increase performance until a point where it levels out. The more trees, however, the more complicated the algorithm. A more complicated algorithm is more resource intensive to use. Generally it is worth adding complexity to the model if it improves performance but we do not want to unnecessarily add complexity.

We can use what is called an Elbow Graph to find the sweet spot. Elbow Graph is a model that optimizes performance without adding unnecessary complexity.

To find the optimal value, let’s do a Grid Search trying all the values from 1 to 100 for n_estimators.

```{python}
n_estimators = list(range(1, 101))
param_grid = {'n_estimators': n_estimators,}
rf = RandomForestClassifier()
gs = GridSearchCV(rf, param_grid, cv=5)
gs.fit(X, y)

```

Instead of just looking at the best params like we did before, we are going to use the entire result from the grid search. The values are located in the cv_results_ attribute. This is a dictionary with a lot of data, however, we will only need one of the keys: mean_test_score. Let’s pull out these values and store them as a variable.

```{python}
scores = gs.cv_results_['mean_test_score']
# [0.91564148, 0.90685413, ...]
```

Now let’s use matplotlib to graph the results.

```{python}
import matplotlib.pyplot as plt

scores = gs.cv_results_['mean_test_score']
plt.plot(n_estimators, scores)
plt.xlabel("n_estimators")
plt.ylabel("accuracy")
plt.xlim(0, 100)
plt.ylim(0.9, 1)
plt.show()
```

If we look at this graph, we see that around 10 trees the graph levels out. The best model occurred at n_estimators=33 and n_estimators=64, but given how volatile it is, that was probably due to random chance. We should choose about 10 to be our number of estimators, because we want the minimum number of estimators that still yield maximum performance.

Now we can build our random forest model with the optimal number of trees.

```{python}
rf = RandomForestClassifier(n_estimators=10)
rf.fit(X, y) 
```

Feature Importances

There are 30 features in the cancer dataset. Does every feature contribute equally to building a model? If not, which subset of features should we use? This is a matter of feature selection.

Random forests provide a straightforward method for feature selection: mean decrease impurity. Recall that a random forest consists of many decision trees, and that for each tree, the node is chosen to split the dataset based on maximum decrease in impurity, typically either Gini impurity or entropy in classification. Thus for a tree, it can be computed how much impurity each feature decreases in a tree. And then for a forest, the impurity decrease from each feature can be averaged. Consider this measure a metric of importance of each feature, we then can rank and select the features according to feature importance.

Scikit-learn provides a feature_importances_ variable with the model, which shows the relative importance of each feature. The scores are scaled down so that the sum of all scores is 1.
Let's find the feature importances in a random forest with n_estimator = 10 using the training dataset, and display them in the descending order.

```{python}
rf = RandomForestClassifier(n_estimators=10, random_state=111)
rf.fit(X_train, y_train)

ft_imp = pd.Series(rf.feature_importances_, index=cancer_data.feature_names).sort_values(ascending=False)
ft_imp.head(10)
```

From the output, we can see that among all features, worst radius is most important (0.31), followed by mean concave points and worst concave points.

New Model on Selected Features

Why should we perform feature selection? Top reasons are: it enables us to train a model faster; it reduces the complexity of a model thus makes it easier to interpret. And if the right subset is chosen, it can improve the accuracy of a model. Choosing the right subset often relies on domain knowledge, some art, and a bit of luck.

In our dataset, we happen to notice that features with "worst" seem to have higher importances. As a result we are going to build a new model with the selected features and see if it improves accuracy. Recall the model from the last part.












