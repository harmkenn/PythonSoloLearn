---
title: "SoloLearn Data Science"
author: "Ken Harmon"
date: "`r format(Sys.time(), '%Y %B %d')`"
output:
  html_document:
    keep_md: yes
    code_folding: hide
    fig_height: 6
    fig_width: 12
    fig_align: center
  pdf_document: default
editor_options:
  chunk_output_type: console
---
 
https://www.sololearn.com/Play/machine-learning

# {.tabset .tabset-fade}

```{r, echo=FALSE}
library(reticulate)
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE)
knitr::knit_engines$set(python = reticulate::eng_python)
reticulate::repl_python()

```

```{r}
#py_install("spyder")
#py_install("sqlalchemy")
```

https://www.sololearn.com/Play/data-science

Welcome to Data Science

Congratulations on taking a big step toward becoming a data scientist!
In addition to working through this course, be sure to take advantage of all of the learning support available to you on SoloLearn, including the daily tips, Try it Yourself practices, code coach challenges, code playground, and engagement with our amazing learner community. We love to hear from you, so please leave comments and feedback as you learn with us.

What is Data Science?

There are many use cases in business for data science including finding a better housing price prediction algorithm for Zillow, finding key attributes associated with wine quality, and building a recommendation system to increase the click-through-rate for Amazon.

Extracting insights from seemingly random data, data science normally involves collecting data, cleaning data, performing exploratory data analysis, building and evaluating machine learning models, and communicating insights to stakeholders.

Why Python?

In this Introduction to Data Science course we’re learning data science with Python. As a general-purpose programming language, Python is now the most popular programming language in data science. It’s easy to use, has great community support, and integrates well with other frameworks (e.g., web applications) in an engineering environment.

This course focuses on exploratory data analysis with three fundamental Python libraries: numpy, pandas and matplotlib. The machine learning library scikit-learn will be covered as well.

In the later modules, we will be predicting home values using linear regression, identifying classes of iris with classification algorithms, and finding clusters within wines, just a few examples of what we can do in data science.
In data science, there are other popular programming languages, such as R, which has an edge in statistical modeling.

Numerical Data

Datasets come from a wide range of sources and formats: it could be collections of numerical measurements, text corpus, images, audio clips, or basically anything. No matter the format, the first step in data science is to transform it into arrays of numbers.

We collected 45 U.S. president heights in centimeters in chronological order and stored them in a list, a built-in data type in python.


```{python}
heights = [189, 170, 189, 163, 183, 171, 185, 168, 173, 183, 173, 173, 175, 178, 183, 193, 178, 173, 174, 183, 183, 180, 168, 180, 170, 178, 182, 180, 183, 178, 182, 188, 175, 179, 183, 193, 182, 183, 177, 185, 188, 188, 182, 185, 191]
```

In this example, George Washington was the first president, and his height was 189 cm.

If we wanted to know how many presidents are taller than 188cm, we could iterate through the list, compare each element against 188, and increase the count by 1 as the criteria is met.

```{python}
cnt = 0
for height in heights:
  if height > 188:
    cnt +=1
print(cnt)
```

This shows that there are five presidents who are taller than 188 cm.

Introduction to Numpy

Numpy (short for Numerical Python) allows us to find the answer to how many presidents are taller than 188cm with ease. Below we show how to use the library and start with the basic object in numpy.

```{python}
import numpy as np
heights_arr = np.array(heights)
print((heights_arr > 188).sum())
```

The import statement allows us to access the functions and modules inside the numpy library. The library will be used frequently, so by convention numpy is imported under a shorter name, np. The second line is to convert the list into a numpy array object, via np.array(), that tools provided in numpy can work with. The last line provides a simple and natural solution, enabled by numpy, to the original question.

As our datasets grow larger and more complicated, numpy allows us the use of a more efficient and for-loop-free method to manipulate and analyze our data. Our dataset example in this module will include the US Presidents' height, age and party.

Size and Shape

An array class in Numpy is called an ndarray or n-dimensional array. We can use this to count the number of presidents in heights_arr, use attribute numpy.ndarray.size:

```{python}
heights_arr.size
```

Note that once an array is created in numpy, its size cannot be changed.

Size tells us how big the array is, shape tells us the dimension. To get current shape of an array use attribute shape:

```{python}
heights_arr.shape
```

The output is a tuple, recall that the built-in data type tuple is immutable whereas a list is mutable, containing a single value, indicating that there is only one dimension, i.e., axis 0. Along axis 0, there are 45 elements (one for each president) Here, heights_arr is a 1d array.
Attribute size in numpy is similar to the built-in method len in python that is used to compute the length of iterable python objects like str, list, dict, etc.

Reshape

Other data we have collected includes the ages of the presidents:


```{python}
ages = [57, 61, 57, 57, 58, 57, 61, 54, 68, 51, 49, 64, 50, 48, 65, 52, 56, 46, 54, 49, 51, 47, 55, 55, 54, 42, 51, 56, 55, 51, 54, 51, 60, 62, 43, 55, 56, 61, 52, 69, 64, 46, 54, 47, 70]
```

Since both heights and ages are all about the same presidents, we can combine them:

```{python}
heights_and_ages = heights + ages 
# convert a list to a numpy array
heights_and_ages_arr = np.array(heights_and_ages)
heights_and_ages_arr.shape
```

This produces one long array. It would be clearer if we could align height and age for each president and reorganize the data into a 2 by 45 matrix where the first row contains all heights and the second row contains ages. To achieve this, a new array can be created by calling numpy.ndarray.reshape with new dimensions specified in a tuple:

```{python}
heights_and_ages_arr.reshape((2,45))
```

The reshaped array is now a 2darray, yet note that the original array is not changed. We can reshape an array in multiple ways, as long as the size of the reshaped array matches that of the original.

```{python}
heights_and_ages_arr = heights_and_ages_arr.reshape((2,45))
```

Data Type

Another characteristic about numpy array is that it is homogeneous, meaning each element must be of the same data type.

For example, in heights_arr, we recorded all heights in whole numbers; thus each element is stored as an integer in the array. To check the data type, use numpy.ndarray.dtype

```{python}
heights_arr.dtype
```

If we mixed a float number in, say, the first element is 189.0 instead of 189

```{python}
heights_float = [189.0, 170, 189, 163, 183, 171, 185, 168, 173, 183, 173, 173, 175, 178, 183, 193, 178, 173, 174, 183, 183, 180, 168, 180, 170, 178, 182, 180, 183, 178, 182, 188, 175, 179, 183, 193, 182, 183, 177, 185, 188, 188, 182, 185, 191]
```

Then after converting the list into an array, we’d see all other numbers are coerced into floats:

```{python}
heights_float_arr = np.array(heights_float)
heights_float_arr
heights_float_arr.dtype
```

Indexing

We can use array indexing to select individual elements from arrays. Like Python lists, numpy index starts from 0.

To access the height of the 3rd president Thomas Jefferson in the 1darray 'heights_arr':

```{python}
heights_arr[2]
```

In a 2darray, there are two axes, axis 0 and 1. Axis 0 runs downward down the rows whereas axis 1 runs horizontally across the columns.

In the 2darrary heights_and_ages_arr, recall that its dimensions are (2, 45). To find Thomas Jefferson’s age at the beginning of his presidency you would need to access the second row where ages are stored:

```{python}
heights_and_ages_arr[1,2]
```

Slicing

What if we want to inspect the first three elements from the first row in a 2darray? We use ":" to select all the elements from the index up to but not including the ending index. This is called slicing.

```{python}
heights_and_ages_arr[0, 0:3]
```

When the starting index is 0, we can omit it as shown below:

```{python}
heights_and_ages_arr[0, :3]
```

What if we’d like to see the entire third column? Specify this by using a ":" as follows

```{python}
heights_and_ages_arr[:, 3]
```

Assigning Single Values

Sometimes you need to change the values of particular elements in the array. For example, we noticed the fourth entry in the heights_arr was incorrect, it should be 165 instead of 163, we can re-assign the correct number by:

```{python}
heights_arr[3] = 165

heights_arr
```

In a 2darray, single values can be assigned easily. You can use indexing for one element. For example, change the fourth entry in heights_arr to 165:

```{python}
heights_and_ages_arr[0, 3] = 165
heights_and_ages_arr
```

Or we can use slicing for multiple elements. For example, to replace the first row by its mean 180 in heights_and_ages_arr:

```{python}
heights_and_ages_arr[0,:] = 180
heights_and_ages_arr
```

We can also combine slicing to change any subset of the array. For example, to reassign 0 to the left upper corner:

```{python}
heights_and_ages_arr[:2, :2] = 0
heights_and_ages_arr
```

Assigning an Array to an Array

In addition, a 1darray or a 2darry can be assigned to a subset of another 2darray, as long as their shapes match. Recall the 2darray heights_and_ages_arr:

```{python}
heights_and_ages_arr
```

If we want to update both height and age of the first president with new data, we can supply the data in a list:

```{python}
heights_and_ages_arr[:, 0] = [190, 58]
heights_and_ages_arr
```

We can also update data in a subarray with a numpy array as such:

```{python}
new_record = np.array([[180, 183, 190], [54, 50, 69]])
heights_and_ages_arr[:, 42:] = new_record
heights_and_ages_arr
```

Combining Two Arrays

Oftentime we obtain data stored in different arrays and we need to combine them into one to keep it in one place. For example, instead of having the ages stored in a list, it could be stored in a 2darray:

```{python}
ages_arr.shape
ages_arr[:3,]
```

If we reshape the heights_arr to (45,1), the same as 'ages_arr', we can stack them horizontally (by column) to get a 2darray using 'hstack':

```{python}
heights_arr = np.array(heights)
heights_arr = heights_arr.reshape((45,1))
ages_arr = np.array(ages)
ages_arr = ages_arr.reshape((45,1))
height_age_arr = np.hstack((heights_arr, ages_arr))
height_age_arr.shape
height_age_arr[:3,]
```

Now height_age_arr has both heights and ages for the presidents, each column corresponds to the height and age of one president.

Similarly, if we want to combine the arrays vertically (by row), we can use 'vstack'.

```{python}
heights_arr = heights_arr.reshape((1,45))
ages_arr = ages_arr.reshape((1,45))

height_age_arr = np.vstack((heights_arr, ages_arr))
height_age_arr.shape
height_age_arr[:,:3]
```

Concatenate

More generally, we can use the function numpy.concatenate. If we want to concatenate, link together, two arrays along rows, then pass 'axis = 1' to achieve the same result as using numpy.hstack; and pass 'axis = 0' if you want to combine arrays vertically.

In the example from the previous part, we were using hstack to combine two arrays horizontally, instead:

```{python}
height_age_arr = np.concatenate((heights_arr, ages_arr), axis=1)
```

Also you can get the same result as using vstack:

```{python}
height_age_arr = np.concatenate((heights_arr, ages_arr), axis=0) 
```

Mathematical Operations on Arrays

Performing mathematical operations on arrays is straightforward. For instance, to convert the heights from centimeters to feet, knowing that 1 centimeter is equal to 0.0328084 feet, we can use multiplication:

```{python}
height_age_arr[:,0]*0.0328084
```

Numpy Array Method

In addition, there are several methods in numpy to perform more complex calculations on arrays. For example, the sum() method finds the sum of all the elements in an array:

```{python}
height_age_arr.sum()
```

The sum of all heights and ages is 10575. In order to sum all heights and sum all ages separately, we can specify axis=0 to calculate the sum across the rows, that is, it computes the sum for each column, or column sum. On the other hand, to obtain the row sums specify axis=1. In this example, we want to calculate the total sum of heights and ages, respectively:

```{python}
height_age_arr.sum(axis=1)
```

The output is the row sums: heights of all presidents (i.e., the first row) add up to 8100, and the sum of ages (i.e., the second row) is 2475.

Comparisons

In practicing data science, we often encounter comparisons to identify rows that match certain values. We can use operations including "<", ">", ">=", "<=", and "==" to do so. For example, in the height_age_arr dataset, we might be interested in only those presidents who started their presidency younger than 55 years old.

```{python}
import numpy as np

heights_arr = np.array([189, 170, 189, 163, 183, 171, 185, 168, 173, 183, 173, 173, 175, 178, 183, 193, 178, 173, 174, 183, 183, 180, 168, 180, 170, 178, 182, 180, 183, 178, 182, 188, 175, 179, 183, 193, 182, 183, 177, 185, 188, 188, 182, 185, 191])
ages_arr = np.array([57, 61, 57, 57, 58, 57, 61, 54, 68, 51, 49, 64, 50, 48, 65, 52, 56, 46, 54, 49, 51, 47, 55, 55, 54, 42, 51, 56, 55, 51, 54, 51, 60, 62, 43, 55, 56, 61, 52, 69, 64, 46, 54, 47, 70]).reshape((-1,1))

heights_arr = heights_arr.reshape((45,1))
height_age_arr = np.hstack((heights_arr, ages_arr))

print(height_age_arr[:, 1] < 55)
```

The output is a 1darray with boolean values that indicates which presidents meet the criteria. If we are only interested in which presidents started their presidency at 51 years of age, we can use "==" instead.

```{python}
height_age_arr[:, 1] == 51
```

Mask & Subsetting

Now that rows matching certain criteria can be identified, a subset of the data can be found. For example, instead of the entire dataset, we want only tall presidents, that is, those presidents whose height is greater than or equal to 182 cm. We first create a mask, 1darray with boolean values:

```{python}
mask = height_age_arr[:, 0] >= 182
mask.sum()
```

Then pass it to the first axis of `height_age_arr` to filter presidents who don’t meet the criteria:

```{python}
tall_presidents = height_age_arr[mask, ]
tall_presidents.shape
```

This is a subarray of height_age_arr, and all presidents in tall_presidents were at least 182cm tall.

Multiple Criteria

We can create a mask satisfying more than one criteria. For example, in addition to height, we want to find those presidents that were 50 years old or younger at the start of their presidency. To achieve this, we use & to separate the conditions and each condition is encapsulated with parentheses "()" as shown below:

```{python}
mask = (height_age_arr[:, 0]>=182) & (height_age_arr[:,1]<=50)
height_age_arr[mask,]
```

The results show us that there are four presidents who satisfy both conditions.

Pandas vs. Numpy

What if we want to inspect the data on Abraham Lincoln in 'height_age_arr' but cannot remember his integer position. Is there a convenient way to access the data by indexing the name of the president like:

```{python}
print(height_age_arr['Abraham Lincoln'])
```

Unfortunately, we will receive an error message. However, it is possible to do this in pandas. The pandas library is built on top of numpy, meaning a lot of features, methods, and functions are shared.

By convention, import the library under a short name "pd":

```{python}
import pandas as pd
```

Series

The Series is one building block in pandas. Pandas Series is a one-dimensional labeled array that can hold data of any type (integer, string, float, python objects, etc.), similar to a column in an excel spreadsheet. The axis labels are collectively called index.

If we are given a bag of letters a, b, and c, and count how many of each we have, we find that there are 1 a, 2 b’s, and 3 c’s. We could create a Series by supplying a list of counts and their corresponding labels:

```{python}
pd.Series([1, 2, 3], index=['a', 'b', 'c']) # with index
```

Alternatively, the values can be a numpy array:

```{python}
pd.Series(np.array([1, 2, 3]), index=['a', 'b', 'c']) # from a 1darray
```

Or, we could use a dictionary to specify the index with keys:

```{python}
pd.Series({'a': 1, 'b': 2, 'c':3}) # from a dict
```

If we don’t specify the index, by default, the index would be the integer positions starting from 0.

In a Series, we can access the value by its index directly:

```{python}
series = pd.Series({'a': 1, 'b': 2, 'c':3})
series['a']
```

Accessing the value by its index, rather than the integer position comes in handy when the dataset is of thousands, if not millions, of rows. Series is the building block for the DataFrame we will introduce next.

DataFrames

In data science, data is usually more than one-dimensional, and of different data types; thus Series is not sufficient. DataFrames are 2darrays with both row and column labels. One way to create a DataFrame from scratch is to pass in a dict. For example, this week, we sold 3 bottles of red wine to Adam, 6 to Bob, and 5 to Charles. We sold 5 bottles of white wine to Adam, 0 to Bob and 10 to Charles. We can organize the data into a DataFrame by creating a dict 'wine_dict' with the number of bottles of each wine type we sold, then pass it along with the customer names as index to create a DataFrame 'sales'.

```{python}
wine_dict = {
  'red_wine': [3, 6, 5], 
  'white_wine':[5, 0, 10]
}
sales = pd.DataFrame(wine_dict, index=["adam", "bob", "charles"])
```

Think of DataFrame as a collection of the Series. Here, sales consists of two Series, one named under "red_wine", the other "white_wine", thus, we can access each series by calling its name:

```{python}
sales['white_wine']
```

Inspect a DataFrame - Shape and Size

Let’s take a look at a new DataFrame, in addition to heights and ages of the presidents, there is information on the order, names and parties. The DataFrame presidents_df is read from a CSV file as follows. Note that index is set to be the names of presidents.

```{python}
import pandas as pd
presidents_df = pd.read_csv("president_heights_party.csv", index_col='name')
```

Similar to numpy, to get the dimensions of a DataFrame, use .shape.

```{python}
presidents_df.shape
```

There are 45 rows and 4 columns in this DataFrame. To get the number of rows we can access the first element in the tuple.

```{python}
presidents_df.shape[0]
```

Size also works on DataFrame to return an integer representing the number of elements in this object.

```{python}
presidents_df.size
```

Inspect a DataFrame - Head and Tail

Instead of looking at the entire dataset, we can just take a peep. To see the first few lines in a DataFrame, use .head(); if we don’t specify n (the number of lines), by default, it displays the first five rows. Here we want to see the top 3 rows.

```{python}
presidents_df.head(n=3)
```

In presidents_df, the index is the name of the president, there are four columns: order, age, height, and party. Similarly, if we want to see the last few rows, we can use .tail(), the default is also five rows.

```{python}
presidents_df.tail()
```

Inspect a DataFrame - Info

Use .info() to get an overview of the DataFrame. Its output includes index, column names, count of non-null values, dtypes, and memory usage.

```{python}
presidents_df.info()
```

The dtype for order, age, and height is integers, while party is an object. The count of non-null values in each column is the same as the number of rows, indicating no missing values.

Rows with .loc

Instead of memorizing the integer positions to locate the order, age, height, and party information of Abraham Lincoln, with DataFrame, we can access it by the name using .loc:

```{python}
presidents_df.loc['Abraham Lincoln']
```

The result is a pandas Series of shape (4,).

```{python}
type(presidents_df.loc['Abraham Lincoln'])

presidents_df.loc['Abraham Lincoln'].shape
```

We can also slice by index. Say we are interested in gathering information on all of the presidents between Abraham Lincoln and Ulysses S. Grant:

```{python}
presidents_df.loc['Abraham Lincoln':'Ulysses S. Grant']
```

Rows with .iloc

Alternatively, if we do know the integer position(s), we can use .iloc to access the row(s).

```{python}
presidents_df.iloc[15]
```

To gather information from the 16th to 18th presidents, we can then:

```{python}
presidents_df.iloc[15:18]
```

Columns

We can retrieve an entire column from presidents_df by name. First we access all the column names:

```{python}
presidents_df.columns
```

Which returns an index object containing all column names. Then we can access the column height by:

```{python}
presidents_df['height']
presidents_df['height'].shape
```

Which returns a Series containing heights from all U.S. presidents.

To select multiple columns, we pass the names in a list, resulting in a DataFrame. Remember, we can use .head() to access the first 3 rows as shown below:

```{python}
presidents_df[['height','age']].head(n=3)
```

More with .loc

If we wanted to access columns order, age, and height, we can do it with .loc. .loc allows us to access any of the columns. For example, if we wanted to access columns from order through height for the first three presidents:

```{python}
presidents_df.loc[:, 'order':'height'].head(n=3)
```

Min / Max / Mean

It’s not practical to print out an entire dataset with a large sample size. Instead, we want to summarize and characterize sample data using only a few values. Summary statistics include measures of location and measures of spread. Measures of location are quantities that represent the average value of a variable while measures of spread represent how similar or dissimilar the values of a variable are.

Measures of Location - Minimum, Maximum, Mean

Measures of Spread - Range, Variance, Standard Deviation

The simplest summary statistics, which are measures of location, include the minimum, the smallest number:

```{python}
presidents_df.min()
presidents_df.max()
presidents_df.mean()
presidents_df.median()
presidents_df.quantile(q=.25)
```

Recall the arithmetic mean is the sum of the elements divided by the number of elements, in python 3.x, division of integers results in a float number.

Once the minimum and maximum are known, we can determine the range, a measure of spread. For example, the height for all U.S. presidents ranges from 163 -- 193 cm.

The mean tells us where the data is centered. For instance, the average age at the start of the presidency is 54.71 years. Note that mean() can only operate on the numeric values, thus the column 'party' was omitted.

Quantiles

Quantiles are cut points dividing the range of the data into continuous intervals with an equal number of observations. Median is the only cut point in 2-quantiles, such that 50% of the data is below the median with the other half above it.

Quartiles let us quickly divide a set of data into four groups, making it easy to see which of the four groups a particular data point is in. Quartiles are then 4-quantiles, that is, 25% of the data are between the minimum and first quartile, the next is 25% between the first quartile and median, the next 25% is between the median and the third quartile, and the last 25% of the data lies between the third quartile and the maximum.

```{python}
presidents_df['age'].quantile([0.25, 0.5, 0.75, 1])
```

Here 25% of presidents started their presidency at 51 years old or younger, while half started their presidency at 55 years old or younger.

Mean and median are usually not of the same value, unless the data is perfectly symmetric. The mean is the average of all the numbers added together and divided by the amount of numbers added. The median is the value separating the higher half from the lower half of the data sample. In the age data, the mean is close to its median, this implies that the data might be symmetric.

```{python}
presidents_df['age'].mean()

presidents_df['age'].median()
```

Variance and Standard Deviation

In probability and statistics, variance is the mean squared deviation of each data point from the mean of the entire dataset.

You can think of it as how far apart a set of numbers are spread out from their average value. Standard deviation (std) is the square root of variance. A high std implies a large spread, and a low std indicates a small spread, or most points are close to the mean.

In one extreme example, the data consists of all constant 2, there is no variation, thus the variation is 0.0, so is its std:

```{python}
const = pd.Series([2, 2, 2])
const.var()
const.std()
```

Lets consider another example:
[2, 3, 4] 

The mean of [2,3,4] is (2+3+4)/3 = 3.0, and its variation is (2-3)^2 + (3-3)^2 + (4-3)^2 = 1+0+1 = 2. Note that in Python, .var() will return the variance divided by N-1 where N is the length of the data, the output is then 2/(3-1) = 1.

```{python}
dat = pd.Series([2, 3, 4])
dat.mean()
dat.var()
dat.std()
```

For the ages of the presidents:

```{python}
presidents_df['age'].var()
presidents_df['age'].std()
```

We can apply std on the entire DataFrame to get column-wise standard deviation.

```{python}
presidents_df.std()
```

describe()

describe() prints out almost all of the summary statistics mentioned previously except for the variance. In addition, it counts all non-null values of each column.

```{python}
presidents_df['age'].describe()
presidents_df.describe()
```

From the output we can see that there are 45 non-null data points of ages, with a mean 55 and std 6.60. The ages range from 42 to 70 with a median 55. Its first and third quartiles are 51 and 58, respectively. Now we have an overall description of all age data. In addition to being applied to a series, describe() can be applied to a DataFrame with multiple columns.

As the count (=45) suggests, there are no null values in any of the three columns. Order is simply an index from 1 to 45. Interestingly, both age and height lie in the interval of roughly the same length, 70-42 = 28 for age while 193-163 = 30 for height. Also both features are of similar standard deviations, indicating a similar spread of the data.

Categorical Variable

The fourth column 'party' was omitted in the output of .describe() because it is a categorical variable. A categorical variable is one that takes on a single value from a limited set of categories. It doesn’t make sense to calculate the mean of democratic, republican, federalist, and other parties. We can check the unique values and corresponding frequency by using .value_counts():

```{python}
presidents_df['party'].value_counts()
```

We can also call .describe() to see that there are 45 non-null values, 7 unique parties, the most frequent party is republican, with a total of 19 presidents belonging to this party.

```{python}
presidents_df['party'].describe()
```

Groupby

Summary statistics on an entire dataset provides a good overall view, but often we’re interested in some calculation conditional upon a given label or category. For example, what is the average height conditional of the presidents party?

To find the value based on a condition, we can use the groupby operation. Think of groupby doing three steps: split, apply, and combine. The split step breaks the DataFrame into multiple DataFrames based on the value of the specified key; the apply step is to perform the operation inside each smaller DataFrame; the last step combines the pieces back into the larger DataFrame.

```{python}
presidents_df.groupby('party')
```

The .groupby("party") returns a DataFrameGroupBy object, not a set of DataFrames. To produce a result, apply an aggregate (.mean()) to this DataFrameGroupBy object:

```{python}
presidents_df.groupby('party').mean()
```

Aggregation

We can also perform multiple operations on the groupby object using .agg() method. It takes a string, a function, or a list thereof. For example, we would like to obtain the min, median, and max values of heights grouped by party:

```{python}
presidents_df.groupby('party')['height'].agg(['min', np.median, max])
```

From the output we can see, the heights of the democratic presidents range from 168 cm to 193 cm, with a median at 180 cm.

Often time we are interested in different summary statistics for multiple columns. For instance, we would like to check the median and mean of heights, but minimum and maximum for ages, grouped by party. In this case, we can pass a dict with key indicate the column name, and value indicate the functions:

```{python}
presidents_df.groupby('party').agg({"height": [np.median, np.mean],"age": [min, max]})
```

Line Plot

Let’s start with a beautiful wave function, sine function, sin(x), where x ranges from 0 to 10. We need to generate the sequence along the x-axis, an evenly spaced array, via linspace().

```{python}
import numpy as np
x = np.linspace(0,10,1000) 
# x is a 1000 evenly spaced numbers from 0 to 10
```

The second line generates an evenly spaced sequence of 1000 numbers o 0 to 10. You can view it as tides go up and down over time and the height of the tides are obtained by the sin function.

```{python}
y = np.sin(x)
```

To plot, as before, we first create the figure and axes objects.

```{python}
import matplotlib.pyplot as plt
fig = plt.figure()
ax = plt.axes()
```

We now make a plot directly from the Axes, "ax.plot()"; by default, it generates a Line2D object. To show the plot, we need to call show().

```{python}
ax.plot(x, y)
plt.show()
```

Alternatively, we can use the pylab interface and let the figure and axes be created for us in the background.

```{python}
plt.plot(x, y)
plt.show()
```

```{python}
x = np.linspace(0, 30, 1000)
plt.cla()
plt.plot(x, np.cos(x))
plt.show()
```

Labels and Titles

One critical component of every figure is the figure title. The job of the title is to accurately communicate what the figure is about. In addition, axes need titles, or more commonly referred to as axis labels. The axis labels explain what the plotted data values are. We can specify the x and y axis labels and a title using plt.xlabel(), plt.ylabel() and plt.title().

```{python}
x = np.linspace(0,10,1000) # 1darray of length 1000
y = np.sin(x)
plt.cla()
plt.plot(x, y)
plt.xlabel('x')
plt.ylabel('y')
plt.title('function sin(x)')
plt.show()
```

Multiple Lines

Usually there are various datasets of similar nature, and we would like to compare them and observe the differences. We can plot multiple lines on the same figure. Say, the sin function capture the tides on the east coast and cos function capture the tides on the west coast at the same time, we can plot them both on the same figure by calling the .plot() function multiple times.

```{python}
x = np.linspace(0,10,1000) # 1darray of length 1000
plt.cla()
plt.plot(x, np.sin(x))
plt.plot(x, np.cos(x))
plt.show()
```

Colors and line styles can be specified to differentiate lines:

```{python}
x = np.linspace(0,10,1000) # 1darray of length 1000
plt.cla()
plt.plot(x, np.sin(x), color='k')
plt.plot(x, np.cos(x), color='r', linestyle ='--')
plt.show()
```

Note that we specified basic colors using a single letter, that is, k for black and r for red. More examples include b for blue, g for green, c for cyan, etc. For more details on the use of colors in matplotlib, refer to its documentation.
https://matplotlib.org/2.0.2/api/colors_api.html

Legend

When there are multiple lines on a single axes, it’s often useful to create a plot legend labeling each line. We can use the method plt.legend(), in conjunction with specifying labels in the plt.plot()

```{python}
x = np.linspace(0,10,1000) # 1darray of length 1000
plt.cla()
plt.plot(x, np.sin(x), 'k:', label='sin(x)')
plt.plot(x, np.cos(x), 'r--', label='cos(x)')
plt.legend()
plt.show()
```

Note here we use 'k:' to indicate the line of sin function to be black (indicated by k) and dotted (indicated by :). Line style and color codes can be combined into a single non-keyword argument in the plt.plot() function.

https://matplotlib.org/3.1.3/tutorials/index.html#colors

Scatter Plot

Another simple plot type is a scatter plot, instead of points being joined by line segments, points are represented individually with a dot, or another shape. Pass 'o' in the plt.plot(), or use plt.scatter() to show the relationship of heights and ages:

```{python}
plt.cla()
plt.scatter(presidents_df['height'], presidents_df['age'])
plt.show()
```

So we created a scatter plot demonstrating the correlation between heights and ages of the presidents. Note that Matplotlib infers the appropriate ranges for us from the data along both x- and y-axis.

By default, each data point is a full circle, and there is a good collection of other shapes. For example, we can pass '<' to draw a triangle pointing to the left, in addition we specify the color to blue:

```{python}
plt.cla()
plt.scatter(presidents_df['height'], presidents_df['age'],
   marker='<',
   color='b')
plt.xlabel('height'); 
plt.ylabel('age')
plt.title('U.S. presidents')
plt.show()
```

https://matplotlib.org/api/markers_api.html?highlight=marker#module-matplotlib.markers

Plotting with Pandas

A great thing about pandas is that it integrates well with matplotlib, so we can plot directly from DataFrames and Series. We specify the kind of plot as 'scatter', 'height' along x-axis, and 'age' along y-axis, and then give it a title:

```{python}
plt.cla()
presidents_df.plot(kind = 'scatter', 
  x = 'height', 
  y = 'age',
  title = 'U.S. presidents')
plt.show()
```

We then created a scatter plot from the DataFrame, with both axis labels and title supplied.
As we specified the x-axis and y-axis with column names from the DataFrame, the labels were also annotated on the axes.

Histogram

A histogram is a diagram that consists of rectangles with width equal to the interval and area proportional to the frequency of a variable. For example, in the histogram below, there are five bins with equal length (i.e., [163, 169), [169, 175), [175, 181), [181, 187), and [187, 193)), and 3 presidents whose height is between 163 cm and 169 cm.

```{python}
plt.cla()
presidents_df['height'].plot(kind='hist',
  title = 'height',
  bins=5)
plt.show()
```

plt.hist(presidents_df['height'], bins=5) will create the same plot as above.

```{python}
plt.cla()
plt.hist(presidents_df['height'], bins=5)
plt.show()
```

In addition, plt.hist() outputs a 1darray with frequency, and bin end points.

Here we see it is a left skewed distribution (the tail of the distribution on the left hand side is longer than on the right hand side) indicating that the mean (180.0 cm) is slightly lower than the median (182.0 cm). The distribution isn’t quite symmetric.
A histogram shows the underlying frequency distribution of a set of continuous data, allowing for inspection of the data for its shape, outliers, skewness, etc.

Boxplot

Remember the output from .describe()?

```{python}
presidents_df['height'].describe()
```

The corresponding boxplot is shown below. The blue box indicates the interquartile range (IQR, between the first quartile and the third), in other words, 50% data fall in this range. The red bar shows the median, and the lower and upper black whiskers are minimum and maximum.

```{python}
plt.cla()
plt.style.use('classic')
presidents_df.boxplot(column='height');
plt.show()
```

As the red bar, the median, cuts the box into unequal parts, it means that the height data is skewed.
The box-and-whisker plot doesn’t show frequency, and it doesn’t display each individual statistic, but it clearly shows where the middle of the data lies and whether the data is skewed.

Bar Plot

Bar plots show the distribution of data over several groups. For example, a bar plot can depict the distribution of presidents by party.

```{python}
plt.cla()
party_cnt = presidents_df['party'].value_counts()

plt.style.use('ggplot')
party_cnt.plot(kind ='bar')
plt.show()
```

Bar plots are commonly confused with a histogram. Histogram presents numerical data whereas bar plot shows categorical data. The histogram is drawn in such a way that there is no gap between the bars, unlike in bar plots.
Making plots directly off pandas Series or DataFrame is powerful. It comes in handy when we explore data analysis in data science.

What is Machine Learning?

Machine learning, a subset of data science, is the scientific study of computational algorithms and statistical models to perform specific tasks through patterns and inference instead of explicit instructions. Machine learning can be described as a set of tools to build models on data. Data scientists explore data, select and build models (machine), tune parameters such that a model fits observations (learning), then use the model to predict and understand aspects of new unseen data.
Machine learning is a set of tools used to build models on data. Building models to understand data and make predictions is an important part of a data scientists' job.

Supervised and Unsupervised Learning

In Machine Learning, we talk about supervised and unsupervised learning. Supervised learning is when we have a known target (also called label) based on past data (for example, predicting what price a house will sell for) and unsupervised learning is when there isn’t a known past answer (for example, determining the topics discussed in restaurant reviews).

In this module we will explore Linear Regression, a supervised machine learning algorithm. In the modules to come we will also explore another supervised machine learning algorithm, classification, as well as an unsupervised machine learning algorithm, clustering.
Both regression and classification problems are supervised learning problems.

Scikit-learn

Scikit-learn, one of the best known machine learning libraries in python for machine learning, implements a large number of commonly used algorithms. Regardless of the type of algorithm, the syntax follows the same workflow: import > instantiate > fit > predict. Once the basic use and syntax of Scikit-learn is understood for one model, switching to a new algorithm is straightforward. Thus for the rest of the course we will be working with scikit-learn to build machine learning models in different use cases.

In this module, we learn how to predict housing prices in Boston, USA using linear regression.
In addition to a set of algorithms, scikit-learn also provides a few small datasets used by the machine learning community to benchmark algorithms on data that comes from the real world, such as boston house-prices dataset that we will be using through this module, iris dataset for classification task in the next, etc.

Linear Regression

We start with linear regression, a simple supervised learning model. Linear regression fits a straight line to data, mathematically:
y = b + m*x

where b is the intercept and m is the slope, x is a feature or an input, whereas y is label or an output. Our job is to find m and b such that the errors are minimized.


To visualize the concept, let’s start with five points (1.5, 2.8), (2, 5.3), (2.5, 5.5), (3, 7), (3.5, 8.8):

We would like to fit a line through these data points, however even by eyeballing it, there doesn’t exist a line going through all five points, so we will do the best we could. What does this mean?

Of the three lines shown below, which one do you think fits the data the best? The green line is y =10 + (-2)*X, the blue line is y = 5.5 + 0*X, and the red line is y = 1 + 2*X:


Red line! Why? Because it best captures the linear relationship between X and y, and it is closest to the points. Mathematically, the distance between the fitted line and data points are calculated by residuals, indicated by the dashed black vertical line in the plot below:


So linear regression essentially is finding the line where it minimizes the sum of the squared residuals that we will discuss later.
Linear regression models are popular because they can perform a fit quickly, and are easily interpreted. Predicting a continuous value with linear regression is a good starting point.

Boston Housing Dataset

The Boston housing dataset is our sample dataset that provides median values of homes in different areas around Boston. Along with the median home values in $1000 (MEDV), crime (CRIM), nitric oxides concentration (NOX), the average number of rooms (RM), percentage of lower status of the population (LSTAT), and other features are provided. Our objective is to predict median home price (MEDV), the target in this scenario, using some features provided.

The data is built in scikit-learn and we will use load_boston to load the object that contains all the information.

```{python}
from sklearn.datasets import load_boston
boston_dataset = load_boston()
```

For easier manipulations later, we create a pandas DataFrame from the numpy ndarrays stored in boston_dataset.data as follows:

```{python}
import pandas as pd
boston = pd.DataFrame(boston_dataset.data,columns=boston_dataset.feature_names)
```

As the name suggests, boston_dataset.feature_names contain names for all features. We then add the target into the DataFrame:

```{python}
boston['MEDV'] = boston_dataset.target
```

There are 506 records, and 14 columns including 13 features and the target.

```{python}
boston.columns
```

https://scikit-learn.org/stable/datasets/index.html#boston-dataset

Head

It is useful for quickly testing if the DataFrame has the right type of data in it. To see the first few rows of a DataFrame, use .head(n), where you can specify n for the number of rows to be selected. If n is omitted, by default, it selects the first 5 rows.

To check the first 5 rows, use boston.head(), for the ease of display, we select columns CHAS, RM, AGE, RAD, and MEDV:

```{python}
boston[['CHAS', 'RM', 'AGE', 'RAD', 'MEDV']].head()
```

After scanning the values, CHAS and RAD appear to be integers, not floats. According to the description of the data, CHAS identifies if the property’s tract bounds a river (=1) or not (=0); and RAD is an accessibility index to radial highways.
Often datasets are loaded from other file formats (e.g., csv, text), it is a good practice to check the first and last few rows of the dataframe and make sure the data is in a consistent format using head and tail, respectively.

Summary Statistics

Recall that It’s not practical to print out an entire dataset with a large sample size. Instead, we want to summarize and characterize sample data using only a few values. To check the summary statistics of the dataset (round to the second decimal place for better display):

```{python}
boston.describe().round(2)
```

Binary CHAS has a mean of 0.07, and its 3rd quartile is 0. This indicates that most of the values in CHAS are 0. The average number of rooms per dwelling ranges from 3.56 to 8.78, with a mean of 6.28 and a median of 6.21. The distribution of RM appears symmetric.
If the DataFrame contains more than just numeric values, by default, describe() outputs the descriptive statistics for the numeric columns. To show the summary statistics of all column, specify include = 'all' in the method.

Visualization

Summary statistics provides a general idea of each feature and the target, but visualization reveals the information more clearly. It’s a good practice to visualize and inspect the distribution column by column. Here we look at CHAS and RM to verify our conclusions from the last part.

```{python}
import matplotlib.pyplot as plt
fig = plt.figure()
boston.hist(column='CHAS')
plt.show()
```

CHAS only takes on two values, 0 and 1, with most of them 0’s. It is consistent with what the describe() reports; specifically, the third quartile of CHAS is 0.

```{python}
plt.cla()
boston.hist(column='RM', bins=20)
plt.show()
```

The distribution of RM appears normal and symmetric. The symmetry aligns with what we observed from the output of describe(), as the mean of RM 6.28 is close to its median 6.21.
Informative data visualization not only reveals insights, but they are invaluable to communicate findings to stakeholders.

Correlation Matrix

To understand the relationship among features (columns), a correlation matrix is very useful in the exploratory data analysis. Correlation measures linear relationships between variables. We can construct a correlation matrix to show correlation coefficients between variables. It is symmetric where each element is a correlation coefficient ranging from -1 and 1. A value near 1 (resp. -1) indicates a strong positive (resp. negative) correlation between variables. We can create a correlation matrix using the "corr" function:

```{python}
corr_matrix = boston.corr().round(2)
```

The last row or column is used to identify features that are most correlated with the target MEDV (median value of owner-occupied homes in $1000’s). LSTAT (percentage of lower status of the population) is most negatively correlated with the target (-0.74) which means that as the percentage of lower status drops, the median house values increases; while RM (the average number of rooms per dwelling) is most positively correlated with MEDV (0.70) which means that the house value increases as the number of rooms increases.
Understanding data using exploratory data analysis is an essential step before building a model. From sample size and distribution to the correlations between features and target, we gather more understanding at each step aiding in feature and algorithm selection.

Data Preparation - Feature Selection

In the previous lesson, we noticed that RM and MEDV are positively correlated. Recall that scatter plot is a useful tool to display the relationship between two features; let’s take a look at the scatter plot:

```{python}
plt.cla()
boston.plot(kind = 'scatter',x = 'RM',y = 'MEDV',figsize=(8,6));
plt.show()
```

We specify the type of the plot by passing a string ‘scatter’ to the argument kind, identify the labels for x and y respectively, and set the size of the figure via a tuple (width, height) in inches.

The price increases as the value of RM increases linearly. There are a few outliers that appear to be outside of the overall pattern. For example, one point on the center right corresponds to a house with almost 9 rooms but a median value slightly above $20K. Homes with similar values usually have around 6 rooms. In addition, the data seems to have a ceiling; that is the maximum median value is capped at 50.

On the other hand prices tend to decrease with an increase in LSTAT; and the trend isn’t as linear.

```{python}
plt.cla()
boston.plot(kind = 'scatter',x = 'LSTAT',y = 'MEDV',figsize=(8,6));
plt.show()
```

Of the two features, RM appears a better choice for predicting MEDV. Thus we start with a univariate linear regression: MEDV = b + m * RM.

In scikit-learn, models require a two-dimensional feature matrix (X, 2darray or a pandas DataFrame) and a one-dimensional target array (Y).

Here we define the feature matrix as the column RM in boston and assign it to X. Note the double brackets around 'RM' in the code below, it is to ensure the result remains a DataFrame, a 2-dimensional data structure:

```{python}
X = boston[['RM']]
print(X.shape) # (506, 1)
```

Similarly, we define our target to be the column MEDV in boston and assign it in a variable called Y:

```{python}
Y = boston['MEDV']
print(Y.shape) # (506, )
```

Recall that the single bracket outputs a Pandas Series, while a double bracket outputs a Pandas DataFrame, and the model expects the feature matrix X to be a 2darray.
Feature selection is used for several reasons, including simplification of models to make them easier to interpret, shorter training time, reducing overfitting, etc.

Instantiating the Model

In scikit-learn, every class of model is represented by a class in python. A class of model is not the same as an instance of a model. Recall that instance is an individual object of a certain class. Thus, we first import the linear regression class, then instantiate the model, that is to create an instance of the class LinearRegression:

```{python}
from sklearn.linear_model import LinearRegression
model = LinearRegression()
```

Now the model is instantiated, but not yet applied to the data.
Scikit-learn makes the distinction between choice of model and application of model to data very clear.

Train-Test Split

Next we split the data into training and testing sets. Why? To assess the performance of the model on newly unseen data. We train the model using a training set, and save the testing set for evaluation.

A good rule of thumb is to split data 70-30, that is, 70% of data is used for training and 30% for testing. We use train_test_split function inside scikit-learn’s module model_selection to split the data into two random subsets. Set random_state so that the results are reproducible.

```{python}
from sklearn.model_selection import train_test_split
X_train, X_test, Y_train, Y_test = train_test_split(X, Y, 
  test_size = 0.3, 
  random_state=1)
```

We check the dimensions to ensure the same number of rows.

```{python}
print(X_train.shape)
print(Y_train.shape)
print(X_test.shape)
print(Y_test.shape)
```

Fitting the Model

In short, fitting is equal to training. It fits the model to the training data and finds the coefficients specified in the linear regression model, i.e., intercept and slope. After it is trained, the model can be used to make predictions.

Now let us apply the model to data. Remember, we save the testing data to report the model performance and only use the training set to build the model. The syntax is:

```{python}
model.fit(X_train, Y_train)
```

The fit() command triggers the computations and the results are stored in the model object.
Fitting is how well the machine learning model measures against the data upon which it was trained.

Parameter Estimates

The linear regression model has been fitted, what it means is that both parameters, the intercept and the slope, have been learned. What are they? In Scikit-learn, by convention all model parameters have trailing underscores, for example to access the estimated intercept from the model, rounded to the 2nd decimal place for better display:

```{python}
model.intercept_.round(2)
# Outputs: -30.57
```

Similarly, the estimated coefficient of feature RM is:

```{python}
model.coef_.round(2)
# Outputs: [8.46]
```

The two parameters represent the intercept and slope of the line fit to the data. Our fitted model is MEDV = -30.57 + 8.46 * RM. For one unit increase in RM, the median home price would go up by $8460.

The full code to fit the model is:

```{python}
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split

X = boston[['RM']]
Y = boston['MEDV']
X_train, X_test, Y_train, Y_test = train_test_split(X, Y, 
  test_size = 0.3,
  random_state=1)
model = LinearRegression()
model.fit(X_train, Y_train)
model.intercept_.round(2)
model.coef_.round(2)
```

You did it! You just built the first linear regression model in scikit-learn: from import the class to instantiate the model, to fit the model to the data, and done!

Prediction

Once the model is trained, supervised machine learning will evaluate test data based on previous predictions for the unseen data. We can make a prediction using the predict() method.

When the average number of rooms per dwelling is 6.5, the model predicts a home value of $24,426.06.

```{python}
import numpy as np
new_RM = np.array([6.5]).reshape(-1,1) # make sure it's 2d
model.predict(new_RM)
# Outputs: [24.42606323]
```

Note that the input has to be 2-dimensional, either a 2darray or DataFrame will work in this case.

This value is the same as we plug in the line b + m*x where b is the estimated intercept from the model, and m is the estimated slope.

```{python}
model.intercept_ + model.coef_*6.5
```

In addition, we can feed the testing set and get predictions for all homes.

```{python}
y_test_predicted = model.predict(X_test)
y_test_predicted.shape
type(y_test_predicted) 
```

# The output is a 1darray, same shape as the Y_test.

```{python}
Y_test.shape
```

The predict() method estimates the median home value by computing model.intercept_ + model.coef_*RM.

Residuals

How good is our prediction? We can examine model performance by visually comparing the fitted line and the true observations in the test set.

```{python}
plt.cla()
plt.scatter(X_test, Y_test,label='testing data');
plt.plot(X_test, y_test_predicted,label='prediction', linewidth=3)
plt.xlabel('RM'); plt.ylabel('MEDV')
plt.legend(loc='upper left')
plt.show()
```

Some points sit on the line, but some are away from it. We can measure the distance between a point to the line along the vertical line, and this distance is referred to as residual or error. A residual is the difference between the observed value of the target and the predicted value. The closer the residual is to 0, the better job our model is doing.

We can calculate a residual and represent it in a scatter plot.

```{python}
residuals = Y_test - y_test_predicted
plt.cla()
# plot the residuals
plt.scatter(X_test, residuals)
# plot a horizontal line at y = 0
plt.hlines(y = 0, 
  xmin = X_test.min(), xmax=X_test.max(),
  linestyle='--')
# set xlim
plt.xlim((4, 9))
plt.xlabel('RM'); plt.ylabel('residuals')
plt.show()
```

Residuals are scattered around the horizontal line, y = 0, with no particular pattern. This seemingly random distribution is a sign that the model is working. Ideally the residuals should be symmetrically and randomly spaced around the horizontal axis; if the residual plot shows some pattern, linear or nonlinear, that’s an indication that our model has room for improvement.
Residual plots can reveal bias from the model and statistical measures indicate goodness-of-fit.

Mean Squared Error

Previously, we learned that when each residual is near 0 it suggests a good fit. For example, the first five residuals in our model:

```{python}
residuals[:5]
```

Those are individual data points, how about the model performance for all data points? We need a way to aggregate the residuals and just report one number as the metric. It is natural to take the average of all residuals:

```{python}
residuals.mean()
# Outputs: -0.236450...
```

-0.24 is quite close to 0, but there’s a problem: residuals can be positive or negative so taking the average cancels them out. That’s not an accurate metric. To solve this, we take a square of each residual, then take the mean of squares. This is called mean squared error (MSE):

```{python}
(residuals**2).mean()
# Outputs: 36.517214...
```

We can also use the mean_squared_error() method under scikit-learn metrics module to output the same result:

```{python}
from sklearn.metrics import mean_squared_error
mean_squared_error(Y_test, y_test_predicted)
# Outputs: 36.517214...
```

In general, the smaller the MSE, the better, yet there is no absolute good or bad threshold. We can define it based on the dependent variable, i.e., MEDV in the test set. Y_test ranges from 6.3 to 50 with a variance 92.26. Compared to the total variance, a MSE of 36.52 is not bad.
To make the scale of errors to be the same as the scale of targets, root mean squared error (RMSE) is often used. It is the square root of MSE.

R-squared

Another common metric to evaluate the model performance is called R-squared; one can calculate it via model.score():

```{python}
model.score(X_test, Y_test)
# 0.6015774471545623
```

It is the proportion of total variation explained by the model. Here, around 60% of variability in the testing data is explained by our model.

The total variation is calculated as the sum of squares of the difference between the response and the mean of response, in the example of testing data:

```{python}
((Y_test-Y_test.mean())**2).sum()
# 13931.482039473683
```

Whereas the variation that the model fails to capture is computed as the sum of squares of residuals:

```{python}
(residuals**2).sum()
# 5550.6166390874705
```

Then the proportion of total variation from the data is:

```{python}
1 - (residuals**2).sum() / ((Y_test-Y_test.mean())**2).sum()
```

A perfect model explains all the variation in the data. Note R-squared is between 0 and 100%: 0% indicates that the model explains none of the variability of the response data around its mean while 100% indicates that the model explains all of it.
Evaluating R-squared values in conjunction with residual plots quantifies model performance.

Overview

Recall LSTAT (% lower status in population) is most negatively correlated to the home price. We can add the feature and build a multivariate linear regression model where the home price depends on both RM and LSTAT linearly:

MEDV = b0 + b1 * RM + b2 * LSTAT
To find intercept b0, and coefficients b1 and b2, all steps are the same except for the data preparation part, we are now dealing with two features:

```{python}
## data preparation
X2 = boston[['RM', 'LSTAT']]
Y = boston['MEDV']
## train test split
## same random_state to ensure the same splits
X2_train, X2_test, Y_train, Y_test = train_test_split(X2, Y,
  test_size = 0.3,
  random_state=1)
model2 = LinearRegression()
model2.fit(X2_train, Y_train)
```

We can access the parameters after model2 is fitted

```{python}
model2.intercept_
model2.coef_
```

Note the coefficients are stored in a 1darray of shape (2,). The second model then is
MEDV = 5.32 + 4.13 * RM + (-0.68) * LSTAT.
Allowing for predictions:

```{python}
y_test_predicted2 = model2.predict(X2_test)
```

The extension from univariate to multivariate linear regression is straightforward in scikit-learn. The model instantiation, fitting, and predictions are identical, the only difference being the data preparation.

Comparing Models

Which model is better? An easy metric for linear regression is the mean squared error (MSE) on the testing data. Better models have lower MSEs. Recall the MSE of the first model on testing data is:

```{python}
mean_squared_error(Y_test, y_test_predicted).round(2)
# Outputs: 36.52
```

The MSE of the second model is:

```{python}
mean_squared_error(Y_test, y_test_predicted2).round(2)
# Outputs: 28.93
```

The second model has a lower MSE, specifically a 21% reduction (36.52-28.93)/36.52 = 21%); thus it does a better job predicting the median home values than the univariate model.
In general, the more features the model includes the lower the MSE would be. Yet be careful about including too many features. Some features could be random noise, thus hurt the interpretability of the model.

##Discrete Values

In the last module, we built a linear regression model to predict a continuous value, the median home value in Boston. In this module, we will work through classification problems whose task is to predict a discrete value.

Discrete data are only able to have certain values, while continuous data can take on any value.

Examples of classification problems involving discrete data values are:
• to predict whether a breast cancer is benign or malignant given a set of features
• to classify an image as containing cats or dogs or horses
• to predict whether an email is spam or not from a given email address

In each of the examples, the labels come in categorical form and represent a finite number of classes.
Discrete data values can be numeric, like the number of students in a class, or it can be categorical, like red, blue or yellow.

##Binary and Multi-class Classification

There are two types of classification: binary and multi-class. If there are two classes to predict, that is a binary classification problem, for example, a benign or malignant tumor. When there are more than two classes, the task is a multi-classification problem. For example, classifying the species of iris, which can be versicolor, virqinica, or setosa, based on their sepal and petal characteristics.

Common algorithms for classification include logistic regression, k nearest neighbors, decision trees, naive bayes, support vector machines, neural networks, etc. Here we will learn how to use k nearest neighbors to classify iris species.
Supervised learning problems are grouped into regression and classification problems. Both problems have as a goal the construction of a mapping function from input variables (X) to an output variable (y). The difference is that the output variable is continuous in regression and categorical for classification.

##Iris Dataset

The famous iris database, first used by Sir R. A. Fisher, is perhaps the best known dataset to be found in pattern recognition literature. There are 150 iris plants, each with 4 numeric attributes: sepal length in cm, sepal width in cm, petal length in cm, and petal width in cm. The task is to predict each plant as an iris-setosa, an iris-versicolor, or an iris-virginica based on these attributes.


The dataset is stored in a csv file, we can load it as a DataFrame using read_csv() in library pandas:

```{python}
import pandas as pd
iris = pd.read_csv('iris.csv')
```

Now inspect the dimensions and first few rows:

```{python}
iris.shape
# (150, 6)
```

We use the .head() function to view the first 5 rows:

```{python}
iris.head()
```

The column id is the row index, not really informative, so we can drop it from the dataset using drop() function:

```{python}
iris.drop('id', axis=1, inplace=True)
iris.head()
```

When we are learning about machine learning algorithms, using simple, well-behaved data such as iris flower dataset, decreases the learning curve and makes it easier to understand and debug.

##Summary Statistics

Check the summary statistics:

```{python}
iris.describe()
```

All four features are numeric, each with different ranges. There are no missing values in any of the columns. Therefore, this is a clean dataset.
The ranges of attributes are still of similar magnitude, thus we will skip standardization. However, standardizing attributes such that each has a mean of zero and a standard deviation of one, can be an important preprocessing step for many machine learning algorithms. This is also called feature scaling; see importance of feature scaling for more details.

https://scikit-learn.org/stable/auto_examples/preprocessing/plot_scaling_importance.html

###Class Distribution

The data set contains 3 classes of 50 instances each. We can check this by:

```{python}
iris.groupby('species').size()
```

Or simply use value_counts():

```{python}
iris['species'].value_counts()
```

The method value_counts() is a great utility for quickly understanding the distribution of the data. When used on the categorical data, it counts the number of unique values in the column of interest.

Iris is a balanced dataset as the data points for each class are evenly distributed.

An example of an imbalanced dataset is fraud. Generally only a small percentage of the total number of transactions is actual fraud, about 1 in 1000. And when the dataset is imbalanced, a slightly different analysis will be used. Therefore, it is important to understand whether the data is balanced or imbalanced.
An imbalanced dataset is one where the classes within the data are not equally represented. To review more on imbalanced data, check out this link.

https://machinelearningmastery.com/tactics-to-combat-imbalanced-classes-in-your-machine-learning-dataset/

###Univariate Plot

To better understand each attribute, start with univariate plots, that is, plots of each individual variable.

```{python}
import matplotlib.pyplot as plt
fig = plt.figure()
iris.hist()
plt.show()
```

This gives us a much clearer idea of the distribution of the input variable, showing that both sepal length and sepal width have a normal (Gaussian) distribution. That is, the distribution has a beautiful symmetric bell shape. However, the length of petals is not normal. Its plot shows two modes, one peak happening near 0 and the other around 5. Less patterns were observed for the petal width.
Histograms are a type of bar chart that displays the counts or relative frequencies of values falling in different class intervals or ranges. There are more univariate summary plots including density plots and boxplots.

###Multivariate Plot

To see the interactions between attributes we use scatter plots. However, it's difficult to see if there's any grouping without any indication of the true species of the flower that a datapoint represents. Therefore, we define a color code for each species to differentiate species visually:

```{python}
# build a dict mapping species to an integer code
inv_name_dict = {'iris-setosa': 0, 
  'iris-versicolor': 1,
  'iris-virginica': 2}
fig = plt.figure()
plt.cla()
# build integer color code 0/1/2
colors = [inv_name_dict[item] for item in iris['species']] 
# scatter plot
scatter = plt.scatter(iris['sepal_len'], iris['sepal_wd'], c = colors)
plt.xlabel('sepal length (cm)')
plt.ylabel('sepal width (cm)')
## add legend
plt.legend(handles=scatter.legend_elements()[0],
  labels = inv_name_dict.keys())
plt.show()
```

Using sepal_length and sepal_width features, we can distinguish iris-setosa from others; separating iris-versicolor from iris-virginica is harder because of the overlap as seen by the green and yellow datapoints.

Similarly, between petal length and width:

```{python}
# scatter plot
plt.cla()
scatter = plt.scatter(iris['petal_len'], iris['petal_wd'],c = colors)
plt.xlabel('petal length (cm)')
plt.ylabel('petal width (cm)')
# add legend
plt.legend(handles= scatter.legend_elements()[0],
  labels = inv_name_dict.keys())
plt.show()
```

Interestingly, the length and width of the petal are highly correlated, and these two features are very useful to identify various iris species. It is notable that the boundary between iris-versicolor and iris-virginica remains a bit fuzzy, indicating the difficulties for some classifiers. It is worth keeping in mind when training to decide which features we should use.
To see scatter plots of all pairs of features, use pandas.plotting.scatter_matrix(). Besides the histograms of individual variables along the diagonal, it will show the scatter plots of all pairs of attributes to help spot structured relationships between features.

###K nearest neighbors

K nearest neighbors (knn) is a supervised machine learning model that takes a data point, looks at its 'k' closest labeled data points, and assigns the label by a majority vote.

Here we see that changing k could affect the output of the model. In knn, k is a hyperparameter. A hyperparameter in machine learning is a parameter whose value is set before the learning process begins. We will learn how to tune the hyperparameter later.

For example, in the figure below, there are two classes: blue squares and red triangles. What label should we assign to the green dot, with unknown label, based on the 3nn algorithm, i.e., when k is 3? Of the 3 closest data points from the green dot (solid line circle), two are red triangles and one is blue square, thus the green dot is predicted to be a red triangle. If k is 5 (dashed line circle), it is then classified as a blue square (3 blue squares versus 2 red triangles, blue squares are the majority).

In scikit-learn, the k nearest neighbors algorithm is implemented in sklearn.neighbors module:

```{python}
from sklearn.neighbors import KNeighborsClassifier
```

All nearest neighbors are iris-setosa (i.e., purple data points); thus by 3-nn, the pointed datum should be labeled as iris-setosa as well.
K nearest neighbors can also be used for regression problems. The difference lies in prediction. Instead of a majority vote, knn for regression makes a prediction using the mean labels of the k closest data points.

###Data Preparation

Earlier we identified that the length and the width of the petals are the most useful features to separate the species; we then define the features and labels as follows:

```{python}
X = iris[['petal_len', 'petal_wd']]
y = iris['species']
```

Recall that to assess the performance of the model, we do so on data that is unseen by the model construction. As a result, we set aside some portion of the data as a test set to mimic the unknown data the model will be presented with in the future. As done in the previous module, we use train_test_split in sklearn.model_selection.

```{python}
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=1, stratify=y)
```

We use a 70-30 split, i.e., 70% of the data is for training and 30% for testing. Note that we specified the split was stratified by label (y). This is done to ensure that the distribution of labels remains similar in both train and test sets:

```{python}
y_train.value_counts()
y_test.value_counts()
```

In classifications, stratified sampling is often chosen to ensure that the train and test sets have approximately the same percentage of samples of each target class as the complete set.

###Modeling

Now we are ready to build and train the model knn. First we import the class of the model:

```{python}
from sklearn.neighbors import KNeighborsClassifier
```

Now create an instance knn from the class KNeighborsClassifier.

```{python}
knn = KNeighborsClassifier(n_neighbors=5)
```

Note that the only parameter we need to set in this problem is n_neighbors, or k as in knn. We set k to be 5 by random choice.

Use the data X_train and y_train to train the model:

```{python}
knn.fit(X_train, y_train)
```

It outputs the trained model. We use most the default values for the parameters, e.g., metric = 'minkowski' and p = 2 together defines that the distance is euclidean distance.
For details on the use of other parameters, refer to scikit-learn documentation.

https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html

##Prediction

###Label Prediction

To make a prediction in scikit learn, we can call the method predict(). We are trying to predict the species of iris using given features in feature matrix X.
Let’s make the predictions on the test data set and save the output in pred for later review:

```{python}
y_pred = knn.predict(X_test)
```

Let’s review the first five predictions:

```{python}
y_pred[:5]
# ['iris-virginica', 'iris-setosa', 'iris-setosa', 'iris-versicolor', 'iris-versicolor']
```

Each prediction is a species of iris and stored in a 1darray.
predict() returns an array of predicted class labels for the predictor data.

###Probability Prediction

Of all classification algorithms implemented in scikit learn, there is an additional method 'predict_prob'. Instead of splitting the label, it outputs the probability for the target in array form. Let’s take a look at what the predicted probabilities are for the 11th and 12th flowers:

```{python}
y_pred_prob = knn.predict_proba(X_test)
y_pred_prob[10:12]
# [[1. , 0. , 0. ],
#  [0. , 0.2, 0.8]]
```

For example, the probability of the 11th flower being predicted an iris-setosa is 1, an iris-versicolor and an iris-virginica are both 0. For the next flower, there is a 20% chance that it would be classified as iris-versicolor but 80% chance to be iris-virginica. What it tells us is that of the five nearest neighbours of the 12th flower in the testing set, 1 is an iris-versicolor, the rest 4 are iris-virginica. To see the corresponding predictions:

```{python}
y_pred[10:12]
# ['iris-setosa', 'iris-virginica']
```

In classification tasks, soft prediction returns the predicted probabilities of data points belonging to each of the classes while hard prediction outputs the labels only.

##Model Evaluation

###Accuracy

In classification the most straightforward metric is accuracy. It calculates the proportion of data points whose predicted labels exactly match the observed labels.

```{python}
(y_pred==y_test.values).sum()
y_test.size
# 44
# 45
```

The classifier made one mistake. Thus, the accuracy is 44/45:

```{python}
(y_pred==y_test.values).sum()/y_test.size
# 0.9777777777777777
```

Same as:

```{python}
knn.score(X_test, y_test)
# 0.9777777777777777
```

Under the module sklearn.metrics, function accuracy_score(y_true, y_pred) does the same calculation.

###Confusion Matrix

Classification accuracy alone can be misleading if there is an unequal number of observations in each class or if there are more than two classes in the dataset. Calculating a confusion matrix will provide a better idea of what the classification is getting right and what types of errors it is making.

What is a confusion matrix? It is a summary of the counts of correct and incorrect predictions, broken down by each class.

In classifying the iris, we can use confusion_matrix() under module sklearn.metrics:

```{python}
from sklearn.metrics import confusion_matrix 
confusion_matrix(y_test, y_pred, labels=['iris-setosa','iris-versicolor','iris-virginica'])
# [[15,  0,  0],
#  [ 0, 15,  0],
#  [ 0,  1, 14]]
```

We can visualize the confusion matrix:

```{python}
from sklearn.metrics import plot_confusion_matrix
plot_confusion_matrix(knn, X_test, y_test, cmap=plt.cm.Blues);
```

Here we specified the labels in order. Each column of the matrix corresponds to a predicted class, and each row corresponds to an actual class. So the row sums up to the total number of instances of the class.

The first row corresponds to the actual iris-setosa; [15, 0, 0] indicates that 15 of iris-setosa are correctly predicted, and none are mislabeled; while the last row [0, 1, 14] suggests that of 15 actual iris-virginica, 0 were predicted as iris-setosa, 1 was predicted to be iris-versicolor, and the remaining 14 were correctly identified as iris-virginica. This is consistent with our observation during exploratory data analysis, that is, there was some overlap between the two species on the scatter plot and it is more difficult to distinguish iris-versicolor from iris-virginica than identifying iris-setosa.
A confusion matrix is a table that is often used to describe the performance of a classification model (or "classifier") on a set of test data for which the true values are known.

```{python}
import numpy as np
y_true = np.array(['dog', 'cat', 'cat', 'dog', 'dog'])
y_pred = np.array(['dog', 'cat', 'cat', 'cat', 'dog'])

confusion_matrix(y_true, y_pred, labels=['cat', 'dog'])
```

###K-fold Cross Validation

Previously we made train-test split before fitting the model so that we can report the model performance on the test data. This is a simple kind of cross validation technique, also known as the holdout method. However, the split is random, as a result, model performance can be sensitive to how the data is split. To overcome this, we introduce k-fold cross validation.

In k fold cross validation, the data is divided into k subsets. Then the holdout method is repeated k times, such that each time, one of the k subsets is used as the test set and the other k-1 subsets are combined to train the model. Then the accuracy is averaged over k trials to provide total effectiveness of the model. In this way, all data points are used; and there are more metrics so we don’t rely on one test data for model performance evaluation.

The simplest way to use k-fold cross-validation in scikit-learn is to call the cross_val_score function on the model and the dataset:

```{python}
from sklearn.model_selection import cross_val_score
# create a new KNN model
knn_cv = KNeighborsClassifier(n_neighbors=3)
```

Note that now we are fitting a 3nn model.

```{python}
# train model with 5-fold cv
cv_scores = cross_val_score(knn_cv, X, y, cv=5)
```

Each of the holdout set contains 20% of the original data.

```{python}
# print each cv score (accuracy) 
print(cv_scores)
#  [0.96666667 0.96666667 0.93333333 0.96666667 1. ]
```

As shown, due to the random assignments, the accuracies on the holdsets fluctuates from 0.9 to 1.

```{python}
# then average them
cv_scores.mean()
# 0.9533333333333334
```

We can not rely on one single train-test split, rather we report that the 3nn model has an accuracy of 95.33% based on a 5-fold cross validation.
As a general rule, 5-fold or 10-fold cross validation is preferred; but there is no formal rule. As k gets larger, the difference in size between the training set and the resampling subsets gets smaller. As this difference decreases, the bias of the technique becomes smaller.

###Grid Search

When we built our first knn model, we set the hyperparameter k to 5, and then to 3 later in k-fold cross validation; random choices really. What is the best k? Finding the optimal k is called tuning the hyperparameter. A handy tool is grid search. In scikit-learn, we use GridSearchCV, which trains our model multiple times on a range of values specified with the param_grid parameter and computes cross validation score, so that we can check which of our values for the tested hyperparameter performed the best.

```{python}
from sklearn.model_selection import GridSearchCV
# create new a knn model
knn2 = KNeighborsClassifier()
# create a dict of all values we want to test for n_neighbors
param_grid = {'n_neighbors': np.arange(2, 10)}
# use gridsearch to test all values for n_neighbors
knn_gscv = GridSearchCV(knn2, param_grid, cv=5)
#fit model to data
knn_gscv.fit(X, y)
```

To check the top performing n_neighbors value:

```{python}
knn_gscv.best_params_
# {'n_neighbors': 4}
```

We can see that 4 is the best value for n_neighbors. What is the accuracy of the model when k is 4?

```{python}
knn_gscv.best_score_
# 0.9666666666666667
```

By using grid search to find the optimal hyperparameter for our model, it improves the model accuracy by over 1%.

Now we are ready to build the final model:

```{python}
knn_final = KNeighborsClassifier(n_neighbors=knn_gscv.best_params_['n_neighbors'])
knn_final.fit(X, y)
y_pred = knn_final.predict(X)
knn_final.score(X, y)
# 0.9733333333333334
```

We can report that our final model, 4nn, has an accuracy of 97.3% in predicting the species of iris!
The techniques of k-fold cross validation and tuning parameters with grid search is applicable to both classification and regression problems.

###Label Prediction with New Data

Now we are ready to deploy the model 'knn_final'. We take some measurements of an iris and record that the length and width of its sepal are 5.84 cm and 3.06 cm, respectively, and the length and width of its petal are 3.76 cm and 1.20 cm, respectively. How do we make a prediction using the built model?

Use model.predict. Since the model was trained on the length and width of petals, that’s the data we will need to make a prediction. Let’s put the petal length and petal width into a numpy array:

```{python}
new_data = np.array([3.76, 1.20])
```

If we feed it to the model:

```{python}
knn_final.predict(np.array(new_data))
# ValueError: Expected 2D array, got 1D array instead
```

Wait, what just happened? When we trained the model, the data is 2D DataFrame, so the model was expecting a 2D array, which could be numpy array or pandas DataFrame. Now new_data is a 1D array, we need to make it 2D as the error message suggested:

```{python}
new_data = new_data.reshape(1, -1)
```

Now we are ready to make a label prediction:

```{python}
knn_final.predict(new_data)
# ['iris-versicolor']
```

Our model predicts that this iris is a versicolor.
Model.predict can also take a 2D list. For example, knn_final.predict([[3.76, 1.2]]) will output the same result as shown in the lesson.

###Probability Prediction with New Data

Let's collect more data: three plants of iris share the same petal width, 2.25cm, but are different in the length of the petal: 5.03 cm, 3.85 cm, and 1.77 cm, respectively. We store the new data into a 2D array as follows:

```{python}
new_data = np.array([[3.76, 1.2], [5.25, 1.2], [1.58, 1.2]])
```

We learned from the previous part that we can make predictions using knn_final.predict():

```{python}
knn_final.predict(new_data)
# (['iris-versicolor', 'iris-virginica', 'iris-setosa']
```

Recall that in classifications, it is more common to predict the probability of each data point being assigned to each label:

```{python}
knn_final.predict_proba(new_data)
# [[0.  , 1.  , 0.  ],
#  [0.  , 0.25, 0.75],
#  [1.  , 0.  , 0.  ]]
```

Each row sums to 1. Take the second iris, our model predicts that there is a probability of 25% that the iris would be versicolor, and 75% virginica. This is consistent with the label prediction: virginica.
For classification algorithms in scikit learn, function predict_proba takes a new data point and outputs a probability for each class as a value between 0 and 1.

```{python}
import numpy as np
from sklearn.metrics import confusion_matrix 
y_true = np.array(['cat', 'dog', 'dog',
  'cat', 'fish', 'dog', 'fish'])
y_pred = np.array(['cat', 'cat', 'cat',
  'cat', 'fish', 'dog', 'fish'])
confusion_matrix(y_true, y_pred,
  labels=['cat', 'dog', 'fish'])
```

##Clustering 

###Overview

Clustering is a type of unsupervised learning that allows us to find groups of similar objects, objects that are more related to each other than to the objects in other groups. This is often used when we don’t have access to the ground truth, in other words, the labels are missing.

Examples of business use cases include the grouping of documents, music, and movies based on their contents, or finding customer segments based on purchase behavior as a basis for recommendation engines.
The goal of clustering is to separate the data into groups, or clusters, with more similar traits to each other than to the data in the other clusters.

###Different Types of Clustering Algorithms

There are more than 100 clustering algorithms known, 12 of them have been implemented in scikit-learn, but few gained popularity.

In general, there are four types:

Centroid based models - each cluster is represented by a single mean vector (e.g., k-means),
Connectivity based models - built based on distance connectivity (e.g., hierarchical clustering)
Distribution based models - built using statistical distributions (e.g., Gaussian mixtures)
Density based models - clusters are defined as dense areas (e.g., DBSCAN)

In this module, we will explore the simple and widely-used clustering algorithm, k-means, to reveal subgroups of wines based on the chemical analysis reports.

For more details on clustering algorithms, refer to documentions.

###K-means

One of the most popular clustering algorithms is k-means. Assuming that there are n data points, the algorithm works as follows:

Step 1: initialization - pick k random points as cluster centers, called centroids
Step 2: cluster assignment - assign each data point to its nearest centroid based on its distance to each centroid, and that forms k clusters
Step 3: centroid updating - for each new cluster, calculate its centroid by taking the average of all the points assigned to the cluster
Step 4: repeat steps 2 and 3 until none of cluster assignments change, or it reaches the maximum number of iterations

The k-means algorithm has been implemented in module sklearn.cluster, to access it:

```{python}
from sklearn.cluster import KMeans
```

The algorithm has gained great popularity because it is easy to implement and scales well to large datasets. However, it is difficult to predict the number of clusters, it can get stuck in local optimums, and it can perform poorly when the clusters are of varying sizes and density.

###Distance Metric

How do we calculate the distance in k-means algorithm? One way is the euclidean distance, a straight line between two data points as shown below.


For example, the euclidean distance between points x1 = (0, 1) and x2 = (2, 0) are given by:


Or in numpy we can calculate the distance as follows:

```{python}
import numpy as np
x1 = np.array([0, 1])
x2 = np.array([2, 0])
print(np.sqrt(((x1-x2)**2).sum()))
# 2.23606797749979
print(np.sqrt(5))
# 2.23606797749979
```

One can extend it to higher dimensions. In the n-dimensional space, there are two points:


Then the euclidean distance from p to q is given by the Pythagorean formula:


There are other distance metrics, such as Manhattan distance, cosine distance, etc. The choice of the distance metric depends on the data.

###Wine Data

In this module, we analyze the result of a chemical analysis of wines grown in a particular region in Italy. And the goal is to try to group similar observations together and determine the number of possible clusters. This would help us make predictions and reduce dimensionality. As we will see there are 13 features for each wine, and if we could group all the wines into, say 3 groups, then it is reducing the 13-dimensional space to a 3-dimensional space. More specifically we can represent each of our original data points in terms of how far it is from each of these three cluster centers.

The analysis reported the quantities of 13 constituents from 178 wines: alcohol, malic acid, ash, alcalinity of ash, magnesium, total phenols, flavanoids, nonflavanoid phenols, proanthocyanins, color intensity, hue, od280/od315 of diluted wines, and proline.

The data is loaded in a dataframe 'wine'.

```{python}
import numpy as numpy 
import pandas as pd
from sklearn.datasets import load_wine

data = load_wine()
wine = pd.DataFrame(data.data, columns=data.feature_names)
print(wine.shape)
print(wine.columns)
wine.shape
#(178, 13)
wine.columns
# Index(['alcohol', 'malic_acid', 'ash', 'alcalinity_of_ash', 'magnesium',
# 'total_phenols', 'flavanoids', 'nonflavanoid_phenols',
# 'proanthocyanins', 'color_intensity', 'hue',
# 'od280/od315_of_diluted_wines', 'proline'],
# dtype='object')
```

For the ease of display, we show the basic statistics of the first 3 features:

```{python}
wine.iloc[:,:3].describe()
```

There are no missing values. It is worth noting that the attributes are not on the same scale. We will have to scale the data later.
Another way to check for column names and the datatype of each column is to use .info().

###Plotting the Data

The summary statistics provide some of the information, while visualization offers a more direct view showing the distribution and the relationship between features.

Here we introduce a plotting function to display histograms along the diagonal and the scatter plots for every pair of attributes off the diagonal, 'scatter_matrix', for the ease of display, let’s show just two features:

```{python}
import matplotlib.pyplot as plt
from pandas.plotting import scatter_matrix
scatter_matrix(wine.iloc[:,[0,5]])
plt.show()
plt.cla()
```

As we don’t know the ground truth, we look into the scatter plots to come up with a reasonable candidate for k, the number of clusters. There seem to be roughly three subgroups. Remember that there are no right or wrong answers for the number of subgroups. In the real world data, rarely do we find clear clusters; but we come up with our best educated guess. For example, in the scatter plot above, there seem to be three subgroups.
No matter whether it is a supervised or unsupervised learning problem, exploratory data analysis (EDA) is essential and strongly recommended before one dives into modeling.

###Pre-processing: Standardization

After examining all the pairs of scatter plot, we pick two features to better illustrate the algorithm: alcohol and total_phenols, whose scatterplot also suggests three subclusters.

```{python}
X = wine[['alcohol', 'total_phenols']]
```

Unlike any supervised learning models, in general, unsupervised machine learning models do not require to split data into training and testing sets since there is no ground truth to validate the model. However, centroid-based algorithms require one pre-processing step because k-means works better on data where each attribute is of similar scales. One way to achieve this is to standardize the data; mathematically:

z = (x - mean) / std

where x is the raw data, mean and std are the average and standard deviation of x, and z is the scaled x such that it is centered at 0 and it has a unit standard deviation. StandardScaler under the sklearn.preprocessing makes it easy:

```{python}
from sklearn.preprocessing import StandardScaler
# instantiate the scaler
scale = StandardScaler()
# compute the mean and std to be used later for scaling
scale.fit(X)
# StandardScaler(copy=True, with_mean=True, with_std=True)
```

We can look into the object scale, extract the calculated mean and std:

```{python}
scale.mean_
# array([13.00061798,  2.29511236])
scale.scale_
# array([0.80954291, 0.62409056])
```

The raw alcohol measurements have a mean 13.00 and std 0.81 while total phenols are centered at 2.29 with a std 0.62. Next we can fit to the training data, and transform it.

```{python}
X_scaled = scale.transform(X)
```

We show the original (red) and scaled (blue) data in the plot to visualize the effect of scaling. After scaling, the data is centered around (0, 0), and the ranges along x- and y-axis are roughly the same, from -2.5 to 2.5.

```{python}
X_scaled.mean(axis=0)
# array([ 7.84141790e-15, -1.95536471e-16])
X_scaled.std(axis=0)
# array([1., 1.])
```

It is a good practice to scale the features before training the model if the algorithms are distance-based. For more details, refer to Importance of Feature Scaling.

###K-means Modeling

Just like linear regression and k nearest neighbours, or any machine learning algorithms in scikit-learn, to do the modeling, we follow instantiate / fit / predict workflow. There are other arguments in KMeans, such as method to initialize the centroids, stopping criteria, etc., yet we focus on the number of clusters, n_clusters, and allow other parameters to take the default values. Here we specify 3 clusters:

```{python}
from sklearn.cluster import KMeans
# instantiate the model
kmeans = KMeans(n_clusters=3)
# fit the model
kmeans.fit(X_scaled)
# make predictions
y_pred = kmeans.predict(X_scaled)
print(y_pred)
```

There are 60 wines in cluster 0, 65 in cluster 1, and 53 in cluster 2.

To inspect the coordinates of the three centroids:

```{python}
kmeans.cluster_centers_
```

A better way to see the results is to visualize them:

```{python}
import matplotlib.pyplot as plt
# plot the scaled data

fig = plt.figure()
plt.cla()
plt.scatter(X_scaled[:,0], 
            X_scaled[:,1],
            c= y_pred)
# identify the centroids
plt.scatter(kmeans.cluster_centers_[:, 0],
            kmeans.cluster_centers_[:, 1], 
            marker="*",
            s = 250, 
            c = [0,1,2], 
            edgecolors='k')
plt.xlabel('alcohol'); plt.ylabel('total phenols')
plt.title('k-means (k=3)')
plt.show()

```

The stars are the centroids. K-means divides wines into three groups: low alcohol but high total phenols (upper right in green), high alcohol and high total phenols (upper left in yellow), and low total phenols (bottom in purple). For any new wine with the chemical report on alcohol and total phenols, we now can classify it based on its distance to each of the centroids. Suppose that there is new wine with alcohol at 13 and total phenols at 2.5, let’s predict which cluster the model will assign the new wine to.

First we need to put the new data into a 2d array:

```{python}
X_new = np.array([[13, 2.5]])
```

Next, we need to standardize the new data:

```{python}
X_new_scaled = scale.transform(X_new)
print(X_new_scaled)
# [[-0.00076337  0.32829793]]
```

Now we are ready to predict the cluster:

```{python}
kmeans.predict(X_new_scaled)
# [1]
```

Indeed among three centroids, the new wine is closest to cluster 1 whose centroid is at (0.07498401, -1.14070754).

Expect to get slightly different results every time you run the code as the order of the clusters might change.
One major shortcoming of k-means is that the random initial guess for the centroids can result in bad clustering, and k-means++ algorithm addresses this obstacle by specifying a procedure to initialize the centroids before proceeding with the standard k-means algorithm. In scikit-learn, the initialization mechanism is set to k-means++, by default.

###Optimal k: The Elbow Method

Can we divide the wines into two subgroups?


No problem, how about four?


Sure! As shown, k-means will be happy to divide the dataset into any integer number of clusters, ranging from 1, an extreme case where all data points belong to one big cluster, to n, another extreme case where each data point is its own cluster.

So which one should we choose, 2, or 3, or 4 for the wines?

Intuitively, k-means problem partitions n data points into k tight sets such that the data points are closer to each other than to the data points in the other clusters. And the tightness can be measured as the sum of squares of the distance from data point to its nearest centroid, or inertia. In scikit-learn, it is stored as inertia_, e.g. when k = 2, the distortion is 185:

```{python}
kmeans = KMeans(n_clusters=2)
kmeans.fit(X_scaled)
kmeans.inertia_
# 185.25081541190127
```

Or when k is 3, the distortion decreases to 114.

```{python}
kmeans = KMeans(n_clusters=3)
kmeans.fit(X_scaled)
kmeans.inertia_
# 114.34674038310786
```

We plot the inertia for different values of k:

```{python}
import numpy as np
plt.cla()
# calculate distortion for a range of number of cluster
inertia = []
for i in np.arange(1, 11):
    km = KMeans(
        n_clusters=i
    )
    km.fit(X_scaled)
    inertia.append(km.inertia_)

# plot
plt.plot(np.arange(1, 11), inertia, marker='o')
plt.xlabel('Number of clusters')
plt.ylabel('Inertia')
plt.show()
```

As the plot shows, the inertia decreases as the number of clusters increases. The optimal k should be where the inertia no longer decreases as rapidly.

For example, k=3 seems to be optimal, as we increase the number of clusters from 3 to 4, the decrease in inertia slows down significantly, compared to that from 2 to 3. This approach is called elbow method (can you see why?). It is a useful graphical tool to estimate the optimal k in k-means.
One single inertia alone is not suitable to determine the optimal k because the larger k is, the lower the inertia will be.

###Modeling With More Features

Previously to build kmeans models, we used two (out of thirteen) features: alcohol and total phenols. The choice is random and it is easy to visualize the results. However, can we use more features, for example all of them? Why not? Let’s try it.

```{python}
X = wine
```

Don’t forget to standardize each feature.

```{python}
scale = StandardScaler() 
scale.fit(X)
X_scaled = scale.transform(X)
```

Plot the inertia for a range of k to determine the optimal k via elbow method:

```{python}
plt.cla()
inertia = [] 
for i in np.arange(1, 11):
    km = KMeans(
        n_clusters=i 
        )
    km.fit(X_scaled) 
    inertia.append(km.inertia_)
plt.plot(np.arange(1, 11), inertia, marker='o')
plt.xlabel('Number of clusters')
plt.ylabel('Inertia')
plt.title("all feature")
plt.show()
```

Similarly we spot that the inertia no longer decreases as rapidly after k = 3. We then finalize the model by setting n_clusters = 3 and obtain the predictions.

```{python}
k_opt = 3
kmeans = KMeans(k_opt)
kmeans.fit(X_scaled)
y_pred = kmeans.predict(X_scaled)
print(y_pred)
```

Compared to the predictions using only two features, the two models produce very similar results. For instance, the first 21 wines are predicted to belong to the same cluster from both models, so are the last 19 wines. In fact, only 13 out of 178 wines were clustered differently by the two models.

It is natural to ask, which model is better? Recall that clustering is an unsupervised learning method, which indicates that we don’t know the ground truth of the labels. Thus it is difficult, if not impossible, to determine that the model with 2 features is more accurate in grouping wines than the one with all 13 features, or vice versa.

Which model, in other words which features, should you choose is often determined by external information. For example, the marketing department wants to know if a continent-specific strategy is needed to sell these wines. We now have access to consumers' demographic information and the three clusters identified from model A correspond better to customers in Europe, Asia, and North America respectively than model B; then model A is the winner. It is an oversimplified example, but you get the gist.
In practice, the features are often chosen by the collaboration between data scientists and domain knowledge experts.








